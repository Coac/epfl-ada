{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Applied ML\n",
    "## Question 1: Propensity score matching\n",
    "### 0. Loading the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSW1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9930.0460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSW2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3595.8940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSW3</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24909.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NSW4</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7506.1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NSW5</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.7899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  treat  age  educ  black  hispan  married  nodegree  re74  re75  \\\n",
       "0  NSW1      1   37    11      1       0        1         1   0.0   0.0   \n",
       "1  NSW2      1   22     9      0       1        0         1   0.0   0.0   \n",
       "2  NSW3      1   30    12      1       0        0         0   0.0   0.0   \n",
       "3  NSW4      1   27    11      1       0        0         1   0.0   0.0   \n",
       "4  NSW5      1   33     8      1       0        0         1   0.0   0.0   \n",
       "\n",
       "         re78  \n",
       "0   9930.0460  \n",
       "1   3595.8940  \n",
       "2  24909.4500  \n",
       "3   7506.1460  \n",
       "4    289.7899  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lalonde_df = pd.read_csv('./lalonde.csv')\n",
    "lalonde_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A naive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2af2005d630>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSeowHCRJHYaDJKnDcJAkdRgOkqSO/wteeEH4jz2EGQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2af1e02f320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(lalonde_df.treat, lalonde_df.re78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: -0.0390327053015\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>non_treated</th>\n",
       "      <th>treated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6984.169742</td>\n",
       "      <td>6349.143530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7294.161791</td>\n",
       "      <td>7867.402218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>220.181300</td>\n",
       "      <td>485.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4975.505000</td>\n",
       "      <td>4232.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11688.820000</td>\n",
       "      <td>9642.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25564.670000</td>\n",
       "      <td>60307.930000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        non_treated       treated\n",
       "count    429.000000    185.000000\n",
       "mean    6984.169742   6349.143530\n",
       "std     7294.161791   7867.402218\n",
       "min        0.000000      0.000000\n",
       "25%      220.181300    485.229800\n",
       "50%     4975.505000   4232.309000\n",
       "75%    11688.820000   9642.999000\n",
       "max    25564.670000  60307.930000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treated_df = lalonde_df[lalonde_df.treat == 1]\n",
    "control_df = lalonde_df[lalonde_df.treat == 0]\n",
    "\n",
    "print(\"Correlation:\", lalonde_df.treat.corr(lalonde_df.re78))\n",
    "\n",
    "pd.DataFrame(dict(treated = treated_df.re78.describe(), \n",
    "                  non_treated=control_df.re78.describe()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive interpretation :  \n",
    "The treatment seems to have no effect to the outcome\n",
    "\n",
    "### 2. A closer look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>185.0</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.816216</td>\n",
       "      <td>10.345946</td>\n",
       "      <td>0.843243</td>\n",
       "      <td>0.059459</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.708108</td>\n",
       "      <td>2095.573689</td>\n",
       "      <td>1532.055314</td>\n",
       "      <td>6349.143530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.155019</td>\n",
       "      <td>2.010650</td>\n",
       "      <td>0.364558</td>\n",
       "      <td>0.237124</td>\n",
       "      <td>0.392722</td>\n",
       "      <td>0.455867</td>\n",
       "      <td>4886.620353</td>\n",
       "      <td>3219.250870</td>\n",
       "      <td>7867.402218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>485.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4232.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1291.468000</td>\n",
       "      <td>1817.284000</td>\n",
       "      <td>9642.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35040.070000</td>\n",
       "      <td>25142.240000</td>\n",
       "      <td>60307.930000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       treat         age        educ       black      hispan     married  \\\n",
       "count  185.0  185.000000  185.000000  185.000000  185.000000  185.000000   \n",
       "mean     1.0   25.816216   10.345946    0.843243    0.059459    0.189189   \n",
       "std      0.0    7.155019    2.010650    0.364558    0.237124    0.392722   \n",
       "min      1.0   17.000000    4.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.0   20.000000    9.000000    1.000000    0.000000    0.000000   \n",
       "50%      1.0   25.000000   11.000000    1.000000    0.000000    0.000000   \n",
       "75%      1.0   29.000000   12.000000    1.000000    0.000000    0.000000   \n",
       "max      1.0   48.000000   16.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree          re74          re75          re78  \n",
       "count  185.000000    185.000000    185.000000    185.000000  \n",
       "mean     0.708108   2095.573689   1532.055314   6349.143530  \n",
       "std      0.455867   4886.620353   3219.250870   7867.402218  \n",
       "min      0.000000      0.000000      0.000000      0.000000  \n",
       "25%      0.000000      0.000000      0.000000    485.229800  \n",
       "50%      1.000000      0.000000      0.000000   4232.309000  \n",
       "75%      1.000000   1291.468000   1817.284000   9642.999000  \n",
       "max      1.000000  35040.070000  25142.240000  60307.930000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treated_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.0</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.030303</td>\n",
       "      <td>10.235431</td>\n",
       "      <td>0.202797</td>\n",
       "      <td>0.142191</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.596737</td>\n",
       "      <td>5619.236506</td>\n",
       "      <td>2466.484443</td>\n",
       "      <td>6984.169742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.786653</td>\n",
       "      <td>2.855238</td>\n",
       "      <td>0.402552</td>\n",
       "      <td>0.349654</td>\n",
       "      <td>0.500419</td>\n",
       "      <td>0.491126</td>\n",
       "      <td>6788.750796</td>\n",
       "      <td>3291.996183</td>\n",
       "      <td>7294.161791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2547.047000</td>\n",
       "      <td>1086.726000</td>\n",
       "      <td>4975.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9277.128000</td>\n",
       "      <td>3881.419000</td>\n",
       "      <td>11688.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25862.320000</td>\n",
       "      <td>18347.230000</td>\n",
       "      <td>25564.670000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       treat         age        educ       black      hispan     married  \\\n",
       "count  429.0  429.000000  429.000000  429.000000  429.000000  429.000000   \n",
       "mean     0.0   28.030303   10.235431    0.202797    0.142191    0.512821   \n",
       "std      0.0   10.786653    2.855238    0.402552    0.349654    0.500419   \n",
       "min      0.0   16.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.0   19.000000    9.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.0   25.000000   11.000000    0.000000    0.000000    1.000000   \n",
       "75%      0.0   35.000000   12.000000    0.000000    0.000000    1.000000   \n",
       "max      0.0   55.000000   18.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree          re74          re75          re78  \n",
       "count  429.000000    429.000000    429.000000    429.000000  \n",
       "mean     0.596737   5619.236506   2466.484443   6984.169742  \n",
       "std      0.491126   6788.750796   3291.996183   7294.161791  \n",
       "min      0.000000      0.000000      0.000000      0.000000  \n",
       "25%      0.000000      0.000000      0.000000    220.181300  \n",
       "50%      1.000000   2547.047000   1086.726000   4975.505000  \n",
       "75%      1.000000   9277.128000   3881.419000  11688.820000  \n",
       "max      1.000000  25862.320000  18347.230000  25564.670000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treated group contains more \"black\" people (84%) than the the control group (only 20%).  \n",
    "Moreover, the treated people were earn less money before the treatment 1974-1975 than the control people.\n",
    "Furthermore, the ratio of married people in the treated group is less than the control group (19% against 51%).\n",
    "Despite of having more or less the same years of education, the treated group has more no degree than the control one.  \n",
    "\n",
    "**It seems that the two groups dont come from the same distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2af202a19e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEKCAYAAAAVaT4rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2af200f29e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAELCAYAAADeNe2OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK6WUw/z/1+rHxlBfL+AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2af201708d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\n",
    "sns.distplot(treated_df.age, ax=ax1)\n",
    "sns.distplot(control_df.age, ax=ax2)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\n",
    "sns.distplot(treated_df.educ, ax=ax1)\n",
    "sns.distplot(control_df.educ, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the two groups dont come frome the same distribution, the naive interpretation is not acceptable.\n",
    "\n",
    "### 3. A propensity score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "logistic= linear_model.LogisticRegression()\n",
    "\n",
    "y = lalonde_df.treat\n",
    "x = lalonde_df.drop(['treat', 'id', 're78'], axis=1)\n",
    "\n",
    "logistic.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "      <th>propensity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSW1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9930.0460</td>\n",
       "      <td>0.443350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSW2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3595.8940</td>\n",
       "      <td>0.144660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSW3</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24909.4500</td>\n",
       "      <td>0.722355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NSW4</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7506.1460</td>\n",
       "      <td>0.664151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NSW5</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.7899</td>\n",
       "      <td>0.698286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  treat  age  educ  black  hispan  married  nodegree  re74  re75  \\\n",
       "0  NSW1      1   37    11      1       0        1         1   0.0   0.0   \n",
       "1  NSW2      1   22     9      0       1        0         1   0.0   0.0   \n",
       "2  NSW3      1   30    12      1       0        0         0   0.0   0.0   \n",
       "3  NSW4      1   27    11      1       0        0         1   0.0   0.0   \n",
       "4  NSW5      1   33     8      1       0        0         1   0.0   0.0   \n",
       "\n",
       "         re78  propensity_score  \n",
       "0   9930.0460          0.443350  \n",
       "1   3595.8940          0.144660  \n",
       "2  24909.4500          0.722355  \n",
       "3   7506.1460          0.664151  \n",
       "4    289.7899          0.698286  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba = logistic.predict_proba(x)\n",
    "propensity_score = proba[:, 1]\n",
    "\n",
    "propensity_score_series = pd.Series(propensity_score)\n",
    "propensity_df = lalonde_df.assign(propensity_score=propensity_score_series.values)\n",
    "propensity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 : Applied Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and loading the data into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_all = fetch_20newsgroups(subset='all')\n",
    "labels_all = data_all.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Raw text                  Category\n",
       "0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...          rec.sport.hockey\n",
       "1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...  comp.sys.ibm.pc.hardware\n",
       "2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...     talk.politics.mideast\n",
       "3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...  comp.sys.ibm.pc.hardware\n",
       "4  From: Alexander Samuel McDiarmid <am2o+@andrew...     comp.sys.mac.hardware"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will print only the first rows\n",
    "data_to_print = data_all.data[0:5]\n",
    "target_to_print = data_all.target[0:5]\n",
    "\n",
    "# Make a pandas dataframe from sklearn dataset\n",
    "df = pd.DataFrame(np.c_[data_to_print, target_to_print], columns= np.append(\"Raw text\", \"Category\"))\n",
    "df.Category = df.Category.map(lambda cat_index: data_all.target_names[int(cat_index)])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the text using TF-IDF features and default Scikit-learn functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has  18846  examples with each examples having  173762  dimensions.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors_all = vectorizer.fit_transform(data_all.data)\n",
    "print('Dataset has ', vectors_all.shape[0] , ' examples with each examples having ', vectors_all.shape[1] , ' dimensions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014526</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.120262</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.120262</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119185</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.120262</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TF-IDF                  Category\n",
       "0  0.014526          rec.sport.hockey\n",
       "1  0.120262  comp.sys.ibm.pc.hardware\n",
       "2  0.120262     talk.politics.mideast\n",
       "3  0.119185  comp.sys.ibm.pc.hardware\n",
       "4  0.120262     comp.sys.mac.hardware"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will print only the first rows\n",
    "data_to_print = vectors_all.data[0:5]\n",
    "target_to_print = data_all.target[0:5]\n",
    "\n",
    "# Make a pandas dataframe from sklearn dataset\n",
    "df = pd.DataFrame(np.c_[data_to_print, target_to_print], columns= np.append(\"TF-IDF\", \"Category\"))\n",
    "df.Category = df.Category.map(lambda cat_index: data_all.target_names[int(cat_index)])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Cross Validation bench for finding the optimal parameter\n",
    "\n",
    "This whole section needs to be run just once for finding the optimal params for andom Forest Classfier. Once found, we can directly jump to the next - Final model section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data splits\n",
    "\n",
    "The training data set is 10% of the complete data where the total cross validation data is 90%. We have used scikit learn functionality for this but the same can also be achieved with few more lines of code by joining the labels with data, slicing using pandas/numpy and separating the labels afterwards.\n",
    "\n",
    "As for cross-validation , we train each model thrice on a randomly chosen subset of the cross-val data. Notice that we have chosen the test_size as 0.11 in Shuffle split ( although called test_size this is the size of the cross_val dataset) because it ~115 of the 90% of the total data. Thus totalling to 9.9%~10% of the whole data. Otherwise, we would've got 9% of the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_cv, vector_test,labels_cv, labels_test = train_test_split( vectors_all, labels_all, test_size=0.1, random_state=42)\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.11, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Starting Cross-Validation\n",
    "\n",
    "Starting with a wide inital guess of params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [20 ,30, 40, 50, 60, 70],\n",
    "              'n_estimators': [5, 10, 15, 20, 30, 40, 50], }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `%time` magical command allows to track the actual time taken to complete. `n_jobs` is an additional parameter \n",
    "to specify the number of cores to use to compute. Can be adjused as per user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=0, test_size=0.1, train_size=None),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=8, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15, 20, 30, 40, 50], 'max_depth': [20, 30, 40, 50, 60, 70]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf =  GridSearchCV(RandomForestClassifier(random_state=0 , n_jobs = 8), param_grid , cv = cv)\n",
    "%time clf.fit(vectors_cv, labels_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining a clean table for performance measures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.485441</td>\n",
       "      <td>0.165653</td>\n",
       "      <td>0.411707</td>\n",
       "      <td>0.487574</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_estimators': 5, 'max_depth': 20}</td>\n",
       "      <td>42</td>\n",
       "      <td>0.424278</td>\n",
       "      <td>0.494824</td>\n",
       "      <td>0.426635</td>\n",
       "      <td>0.495545</td>\n",
       "      <td>0.384207</td>\n",
       "      <td>0.472353</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.006569</td>\n",
       "      <td>0.019469</td>\n",
       "      <td>0.010767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.117120</td>\n",
       "      <td>0.156975</td>\n",
       "      <td>0.509134</td>\n",
       "      <td>0.625590</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 20}</td>\n",
       "      <td>39</td>\n",
       "      <td>0.503241</td>\n",
       "      <td>0.623755</td>\n",
       "      <td>0.532115</td>\n",
       "      <td>0.634762</td>\n",
       "      <td>0.492045</td>\n",
       "      <td>0.618252</td>\n",
       "      <td>0.125276</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.016881</td>\n",
       "      <td>0.006864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.245847</td>\n",
       "      <td>0.151944</td>\n",
       "      <td>0.574150</td>\n",
       "      <td>0.701083</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_estimators': 15, 'max_depth': 20}</td>\n",
       "      <td>35</td>\n",
       "      <td>0.575722</td>\n",
       "      <td>0.700865</td>\n",
       "      <td>0.591632</td>\n",
       "      <td>0.705516</td>\n",
       "      <td>0.555097</td>\n",
       "      <td>0.696868</td>\n",
       "      <td>0.084782</td>\n",
       "      <td>0.008742</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.003534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.826435</td>\n",
       "      <td>0.149638</td>\n",
       "      <td>0.624435</td>\n",
       "      <td>0.751114</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 20}</td>\n",
       "      <td>32</td>\n",
       "      <td>0.629935</td>\n",
       "      <td>0.754782</td>\n",
       "      <td>0.635828</td>\n",
       "      <td>0.751310</td>\n",
       "      <td>0.607543</td>\n",
       "      <td>0.747248</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>0.012219</td>\n",
       "      <td>0.012185</td>\n",
       "      <td>0.003079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.272358</td>\n",
       "      <td>0.140097</td>\n",
       "      <td>0.675309</td>\n",
       "      <td>0.806560</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 20}</td>\n",
       "      <td>26</td>\n",
       "      <td>0.674131</td>\n",
       "      <td>0.797956</td>\n",
       "      <td>0.676488</td>\n",
       "      <td>0.810273</td>\n",
       "      <td>0.675309</td>\n",
       "      <td>0.811452</td>\n",
       "      <td>0.071779</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.006103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.004456</td>\n",
       "      <td>0.204342</td>\n",
       "      <td>0.709094</td>\n",
       "      <td>0.833923</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 20}</td>\n",
       "      <td>20</td>\n",
       "      <td>0.705952</td>\n",
       "      <td>0.829468</td>\n",
       "      <td>0.715380</td>\n",
       "      <td>0.839557</td>\n",
       "      <td>0.705952</td>\n",
       "      <td>0.832744</td>\n",
       "      <td>0.294505</td>\n",
       "      <td>0.049824</td>\n",
       "      <td>0.004445</td>\n",
       "      <td>0.004202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.567097</td>\n",
       "      <td>0.241132</td>\n",
       "      <td>0.725594</td>\n",
       "      <td>0.850585</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 20}</td>\n",
       "      <td>19</td>\n",
       "      <td>0.723630</td>\n",
       "      <td>0.844864</td>\n",
       "      <td>0.723041</td>\n",
       "      <td>0.854232</td>\n",
       "      <td>0.730112</td>\n",
       "      <td>0.852660</td>\n",
       "      <td>0.138987</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>0.004096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.885443</td>\n",
       "      <td>0.161891</td>\n",
       "      <td>0.465527</td>\n",
       "      <td>0.603249</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_estimators': 5, 'max_depth': 30}</td>\n",
       "      <td>41</td>\n",
       "      <td>0.469063</td>\n",
       "      <td>0.609342</td>\n",
       "      <td>0.467885</td>\n",
       "      <td>0.599515</td>\n",
       "      <td>0.459635</td>\n",
       "      <td>0.600891</td>\n",
       "      <td>0.044893</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.004345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.853358</td>\n",
       "      <td>0.161247</td>\n",
       "      <td>0.576704</td>\n",
       "      <td>0.747773</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 30}</td>\n",
       "      <td>34</td>\n",
       "      <td>0.577490</td>\n",
       "      <td>0.746004</td>\n",
       "      <td>0.578079</td>\n",
       "      <td>0.749017</td>\n",
       "      <td>0.574543</td>\n",
       "      <td>0.748297</td>\n",
       "      <td>0.065387</td>\n",
       "      <td>0.009129</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.993183</td>\n",
       "      <td>0.137166</td>\n",
       "      <td>0.641917</td>\n",
       "      <td>0.814968</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_estimators': 15, 'max_depth': 30}</td>\n",
       "      <td>30</td>\n",
       "      <td>0.626400</td>\n",
       "      <td>0.811386</td>\n",
       "      <td>0.654685</td>\n",
       "      <td>0.812959</td>\n",
       "      <td>0.644667</td>\n",
       "      <td>0.820558</td>\n",
       "      <td>0.172783</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>0.004005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.801513</td>\n",
       "      <td>0.139816</td>\n",
       "      <td>0.679041</td>\n",
       "      <td>0.852179</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 30}</td>\n",
       "      <td>25</td>\n",
       "      <td>0.677077</td>\n",
       "      <td>0.849515</td>\n",
       "      <td>0.674131</td>\n",
       "      <td>0.850956</td>\n",
       "      <td>0.685916</td>\n",
       "      <td>0.856067</td>\n",
       "      <td>0.127854</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>0.002811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.583630</td>\n",
       "      <td>0.216298</td>\n",
       "      <td>0.731683</td>\n",
       "      <td>0.893453</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 30}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.727166</td>\n",
       "      <td>0.890592</td>\n",
       "      <td>0.719505</td>\n",
       "      <td>0.892361</td>\n",
       "      <td>0.748379</td>\n",
       "      <td>0.897406</td>\n",
       "      <td>0.282430</td>\n",
       "      <td>0.062495</td>\n",
       "      <td>0.012213</td>\n",
       "      <td>0.002887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.139858</td>\n",
       "      <td>0.216382</td>\n",
       "      <td>0.750147</td>\n",
       "      <td>0.911513</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 30}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.743665</td>\n",
       "      <td>0.910443</td>\n",
       "      <td>0.746022</td>\n",
       "      <td>0.910443</td>\n",
       "      <td>0.760754</td>\n",
       "      <td>0.913653</td>\n",
       "      <td>0.253957</td>\n",
       "      <td>0.063661</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>0.001513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.571270</td>\n",
       "      <td>0.195859</td>\n",
       "      <td>0.765272</td>\n",
       "      <td>0.922454</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 30}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.761933</td>\n",
       "      <td>0.923611</td>\n",
       "      <td>0.748379</td>\n",
       "      <td>0.918436</td>\n",
       "      <td>0.785504</td>\n",
       "      <td>0.925314</td>\n",
       "      <td>0.264881</td>\n",
       "      <td>0.051579</td>\n",
       "      <td>0.015339</td>\n",
       "      <td>0.002925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.920082</td>\n",
       "      <td>0.147718</td>\n",
       "      <td>0.487331</td>\n",
       "      <td>0.692807</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_estimators': 5, 'max_depth': 40}</td>\n",
       "      <td>40</td>\n",
       "      <td>0.507955</td>\n",
       "      <td>0.706237</td>\n",
       "      <td>0.480259</td>\n",
       "      <td>0.687828</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.684355</td>\n",
       "      <td>0.795735</td>\n",
       "      <td>0.020983</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>0.009602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.422347</td>\n",
       "      <td>0.142671</td>\n",
       "      <td>0.605578</td>\n",
       "      <td>0.823157</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 40}</td>\n",
       "      <td>33</td>\n",
       "      <td>0.610489</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.604596</td>\n",
       "      <td>0.820362</td>\n",
       "      <td>0.601650</td>\n",
       "      <td>0.818920</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.005007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.739362</td>\n",
       "      <td>0.133749</td>\n",
       "      <td>0.666470</td>\n",
       "      <td>0.878887</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_estimators': 15, 'max_depth': 40}</td>\n",
       "      <td>27</td>\n",
       "      <td>0.663524</td>\n",
       "      <td>0.878538</td>\n",
       "      <td>0.667649</td>\n",
       "      <td>0.880831</td>\n",
       "      <td>0.668238</td>\n",
       "      <td>0.877293</td>\n",
       "      <td>0.201872</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.001465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.070293</td>\n",
       "      <td>0.126705</td>\n",
       "      <td>0.699273</td>\n",
       "      <td>0.910159</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 40}</td>\n",
       "      <td>23</td>\n",
       "      <td>0.693577</td>\n",
       "      <td>0.910705</td>\n",
       "      <td>0.693577</td>\n",
       "      <td>0.912343</td>\n",
       "      <td>0.710666</td>\n",
       "      <td>0.907429</td>\n",
       "      <td>1.048325</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.008056</td>\n",
       "      <td>0.002043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.703439</td>\n",
       "      <td>0.124589</td>\n",
       "      <td>0.743272</td>\n",
       "      <td>0.936648</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 40}</td>\n",
       "      <td>14</td>\n",
       "      <td>0.747201</td>\n",
       "      <td>0.933766</td>\n",
       "      <td>0.736005</td>\n",
       "      <td>0.939531</td>\n",
       "      <td>0.746612</td>\n",
       "      <td>0.936648</td>\n",
       "      <td>0.273186</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.002354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.646753</td>\n",
       "      <td>0.215155</td>\n",
       "      <td>0.769790</td>\n",
       "      <td>0.949358</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 40}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.773718</td>\n",
       "      <td>0.948244</td>\n",
       "      <td>0.758986</td>\n",
       "      <td>0.950668</td>\n",
       "      <td>0.776665</td>\n",
       "      <td>0.949161</td>\n",
       "      <td>0.534332</td>\n",
       "      <td>0.069268</td>\n",
       "      <td>0.007733</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.960682</td>\n",
       "      <td>0.273598</td>\n",
       "      <td>0.786290</td>\n",
       "      <td>0.957656</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.796111</td>\n",
       "      <td>0.957613</td>\n",
       "      <td>0.774308</td>\n",
       "      <td>0.957023</td>\n",
       "      <td>0.788450</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.170681</td>\n",
       "      <td>0.060580</td>\n",
       "      <td>0.009031</td>\n",
       "      <td>0.000536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.748060</td>\n",
       "      <td>0.148117</td>\n",
       "      <td>0.517384</td>\n",
       "      <td>0.759674</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_estimators': 5, 'max_depth': 50}</td>\n",
       "      <td>38</td>\n",
       "      <td>0.535062</td>\n",
       "      <td>0.772209</td>\n",
       "      <td>0.525633</td>\n",
       "      <td>0.755962</td>\n",
       "      <td>0.491456</td>\n",
       "      <td>0.750852</td>\n",
       "      <td>0.674180</td>\n",
       "      <td>0.010698</td>\n",
       "      <td>0.018734</td>\n",
       "      <td>0.009106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.420199</td>\n",
       "      <td>0.168990</td>\n",
       "      <td>0.630328</td>\n",
       "      <td>0.877511</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 50}</td>\n",
       "      <td>31</td>\n",
       "      <td>0.635239</td>\n",
       "      <td>0.880372</td>\n",
       "      <td>0.639364</td>\n",
       "      <td>0.874410</td>\n",
       "      <td>0.616382</td>\n",
       "      <td>0.877752</td>\n",
       "      <td>0.190985</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>0.002440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.748087</td>\n",
       "      <td>0.130341</td>\n",
       "      <td>0.688077</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_estimators': 15, 'max_depth': 50}</td>\n",
       "      <td>24</td>\n",
       "      <td>0.684148</td>\n",
       "      <td>0.923415</td>\n",
       "      <td>0.698291</td>\n",
       "      <td>0.923808</td>\n",
       "      <td>0.681791</td>\n",
       "      <td>0.926363</td>\n",
       "      <td>0.033240</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.007286</td>\n",
       "      <td>0.001307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.511441</td>\n",
       "      <td>0.131921</td>\n",
       "      <td>0.726183</td>\n",
       "      <td>0.946344</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 50}</td>\n",
       "      <td>18</td>\n",
       "      <td>0.720094</td>\n",
       "      <td>0.946803</td>\n",
       "      <td>0.724808</td>\n",
       "      <td>0.945689</td>\n",
       "      <td>0.733648</td>\n",
       "      <td>0.946541</td>\n",
       "      <td>0.065602</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.000475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.752810</td>\n",
       "      <td>0.123906</td>\n",
       "      <td>0.761540</td>\n",
       "      <td>0.964776</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 50}</td>\n",
       "      <td>12</td>\n",
       "      <td>0.758397</td>\n",
       "      <td>0.965671</td>\n",
       "      <td>0.753683</td>\n",
       "      <td>0.963968</td>\n",
       "      <td>0.772540</td>\n",
       "      <td>0.964688</td>\n",
       "      <td>0.161436</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.000698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.149021</td>\n",
       "      <td>0.213565</td>\n",
       "      <td>0.776665</td>\n",
       "      <td>0.974035</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 50}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.765468</td>\n",
       "      <td>0.974384</td>\n",
       "      <td>0.776665</td>\n",
       "      <td>0.972419</td>\n",
       "      <td>0.787861</td>\n",
       "      <td>0.975301</td>\n",
       "      <td>0.077515</td>\n",
       "      <td>0.064190</td>\n",
       "      <td>0.009142</td>\n",
       "      <td>0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.674253</td>\n",
       "      <td>0.194212</td>\n",
       "      <td>0.792772</td>\n",
       "      <td>0.978315</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 50}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.789629</td>\n",
       "      <td>0.978774</td>\n",
       "      <td>0.787861</td>\n",
       "      <td>0.977136</td>\n",
       "      <td>0.800825</td>\n",
       "      <td>0.979036</td>\n",
       "      <td>0.998968</td>\n",
       "      <td>0.047472</td>\n",
       "      <td>0.005740</td>\n",
       "      <td>0.000841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.130178</td>\n",
       "      <td>0.155223</td>\n",
       "      <td>0.537419</td>\n",
       "      <td>0.810731</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_estimators': 5, 'max_depth': 60}</td>\n",
       "      <td>36</td>\n",
       "      <td>0.549794</td>\n",
       "      <td>0.830123</td>\n",
       "      <td>0.533883</td>\n",
       "      <td>0.798546</td>\n",
       "      <td>0.528580</td>\n",
       "      <td>0.803525</td>\n",
       "      <td>0.136971</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.013862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.734597</td>\n",
       "      <td>0.171287</td>\n",
       "      <td>0.658417</td>\n",
       "      <td>0.912277</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 60}</td>\n",
       "      <td>28</td>\n",
       "      <td>0.653506</td>\n",
       "      <td>0.917191</td>\n",
       "      <td>0.667060</td>\n",
       "      <td>0.904481</td>\n",
       "      <td>0.654685</td>\n",
       "      <td>0.915160</td>\n",
       "      <td>0.041833</td>\n",
       "      <td>0.005858</td>\n",
       "      <td>0.006130</td>\n",
       "      <td>0.005575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.226751</td>\n",
       "      <td>0.133294</td>\n",
       "      <td>0.705559</td>\n",
       "      <td>0.948703</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_estimators': 15, 'max_depth': 60}</td>\n",
       "      <td>22</td>\n",
       "      <td>0.695345</td>\n",
       "      <td>0.952503</td>\n",
       "      <td>0.723041</td>\n",
       "      <td>0.943265</td>\n",
       "      <td>0.698291</td>\n",
       "      <td>0.950341</td>\n",
       "      <td>0.075408</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.012420</td>\n",
       "      <td>0.003945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.393644</td>\n",
       "      <td>0.125001</td>\n",
       "      <td>0.737183</td>\n",
       "      <td>0.967178</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 60}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.728933</td>\n",
       "      <td>0.970846</td>\n",
       "      <td>0.741308</td>\n",
       "      <td>0.964819</td>\n",
       "      <td>0.741308</td>\n",
       "      <td>0.965867</td>\n",
       "      <td>0.048445</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>0.002629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7.703230</td>\n",
       "      <td>0.173632</td>\n",
       "      <td>0.773915</td>\n",
       "      <td>0.978883</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 60}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.780790</td>\n",
       "      <td>0.980870</td>\n",
       "      <td>0.768415</td>\n",
       "      <td>0.977725</td>\n",
       "      <td>0.772540</td>\n",
       "      <td>0.978053</td>\n",
       "      <td>0.156272</td>\n",
       "      <td>0.072542</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.001412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9.351722</td>\n",
       "      <td>0.124458</td>\n",
       "      <td>0.792575</td>\n",
       "      <td>0.984954</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 60}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.802004</td>\n",
       "      <td>0.986832</td>\n",
       "      <td>0.786093</td>\n",
       "      <td>0.982835</td>\n",
       "      <td>0.789629</td>\n",
       "      <td>0.985194</td>\n",
       "      <td>0.127502</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.006821</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10.496654</td>\n",
       "      <td>0.257629</td>\n",
       "      <td>0.804164</td>\n",
       "      <td>0.987552</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 60}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.806718</td>\n",
       "      <td>0.988994</td>\n",
       "      <td>0.799057</td>\n",
       "      <td>0.985718</td>\n",
       "      <td>0.806718</td>\n",
       "      <td>0.987945</td>\n",
       "      <td>1.254174</td>\n",
       "      <td>0.018957</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.001366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3.783091</td>\n",
       "      <td>0.149218</td>\n",
       "      <td>0.532115</td>\n",
       "      <td>0.850935</td>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_estimators': 5, 'max_depth': 70}</td>\n",
       "      <td>37</td>\n",
       "      <td>0.532705</td>\n",
       "      <td>0.861701</td>\n",
       "      <td>0.532115</td>\n",
       "      <td>0.841916</td>\n",
       "      <td>0.531526</td>\n",
       "      <td>0.849188</td>\n",
       "      <td>0.172407</td>\n",
       "      <td>0.011259</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.008171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.747668</td>\n",
       "      <td>0.154155</td>\n",
       "      <td>0.646042</td>\n",
       "      <td>0.941060</td>\n",
       "      <td>70</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 70}</td>\n",
       "      <td>29</td>\n",
       "      <td>0.637006</td>\n",
       "      <td>0.944182</td>\n",
       "      <td>0.653506</td>\n",
       "      <td>0.935469</td>\n",
       "      <td>0.647613</td>\n",
       "      <td>0.943527</td>\n",
       "      <td>0.659774</td>\n",
       "      <td>0.020343</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>0.003962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.749373</td>\n",
       "      <td>0.138602</td>\n",
       "      <td>0.705952</td>\n",
       "      <td>0.970017</td>\n",
       "      <td>70</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_estimators': 15, 'max_depth': 70}</td>\n",
       "      <td>21</td>\n",
       "      <td>0.692398</td>\n",
       "      <td>0.972681</td>\n",
       "      <td>0.715969</td>\n",
       "      <td>0.967309</td>\n",
       "      <td>0.709487</td>\n",
       "      <td>0.970060</td>\n",
       "      <td>0.126266</td>\n",
       "      <td>0.023520</td>\n",
       "      <td>0.009942</td>\n",
       "      <td>0.002193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6.762719</td>\n",
       "      <td>0.140030</td>\n",
       "      <td>0.738165</td>\n",
       "      <td>0.981350</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 70}</td>\n",
       "      <td>15</td>\n",
       "      <td>0.725987</td>\n",
       "      <td>0.983491</td>\n",
       "      <td>0.743076</td>\n",
       "      <td>0.979756</td>\n",
       "      <td>0.745433</td>\n",
       "      <td>0.980805</td>\n",
       "      <td>0.792023</td>\n",
       "      <td>0.023083</td>\n",
       "      <td>0.008665</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>8.987674</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.778629</td>\n",
       "      <td>0.988994</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 70}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.775486</td>\n",
       "      <td>0.990959</td>\n",
       "      <td>0.772540</td>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.787861</td>\n",
       "      <td>0.986701</td>\n",
       "      <td>0.132610</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.006638</td>\n",
       "      <td>0.001754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10.700832</td>\n",
       "      <td>0.258806</td>\n",
       "      <td>0.797682</td>\n",
       "      <td>0.992444</td>\n",
       "      <td>70</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 70}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.789629</td>\n",
       "      <td>0.993449</td>\n",
       "      <td>0.797879</td>\n",
       "      <td>0.992859</td>\n",
       "      <td>0.805539</td>\n",
       "      <td>0.991025</td>\n",
       "      <td>0.155441</td>\n",
       "      <td>0.019988</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>0.001032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>13.170077</td>\n",
       "      <td>0.258905</td>\n",
       "      <td>0.807307</td>\n",
       "      <td>0.993470</td>\n",
       "      <td>70</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 70}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800236</td>\n",
       "      <td>0.994693</td>\n",
       "      <td>0.806718</td>\n",
       "      <td>0.993776</td>\n",
       "      <td>0.814968</td>\n",
       "      <td>0.991942</td>\n",
       "      <td>0.082882</td>\n",
       "      <td>0.021130</td>\n",
       "      <td>0.006029</td>\n",
       "      <td>0.001144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0        1.485441         0.165653         0.411707          0.487574   \n",
       "1        2.117120         0.156975         0.509134          0.625590   \n",
       "2        2.245847         0.151944         0.574150          0.701083   \n",
       "3        2.826435         0.149638         0.624435          0.751114   \n",
       "4        3.272358         0.140097         0.675309          0.806560   \n",
       "5        4.004456         0.204342         0.709094          0.833923   \n",
       "6        4.567097         0.241132         0.725594          0.850585   \n",
       "7        1.885443         0.161891         0.465527          0.603249   \n",
       "8        2.853358         0.161247         0.576704          0.747773   \n",
       "9        2.993183         0.137166         0.641917          0.814968   \n",
       "10       3.801513         0.139816         0.679041          0.852179   \n",
       "11       4.583630         0.216298         0.731683          0.893453   \n",
       "12       5.139858         0.216382         0.750147          0.911513   \n",
       "13       5.571270         0.195859         0.765272          0.922454   \n",
       "14       1.920082         0.147718         0.487331          0.692807   \n",
       "15       3.422347         0.142671         0.605578          0.823157   \n",
       "16       4.739362         0.133749         0.666470          0.878887   \n",
       "17       4.070293         0.126705         0.699273          0.910159   \n",
       "18       5.703439         0.124589         0.743272          0.936648   \n",
       "19       7.646753         0.215155         0.769790          0.949358   \n",
       "20       8.960682         0.273598         0.786290          0.957656   \n",
       "21       2.748060         0.148117         0.517384          0.759674   \n",
       "22       4.420199         0.168990         0.630328          0.877511   \n",
       "23       4.748087         0.130341         0.688077          0.924528   \n",
       "24       5.511441         0.131921         0.726183          0.946344   \n",
       "25       6.752810         0.123906         0.761540          0.964776   \n",
       "26       8.149021         0.213565         0.776665          0.974035   \n",
       "27       8.674253         0.194212         0.792772          0.978315   \n",
       "28       3.130178         0.155223         0.537419          0.810731   \n",
       "29       4.734597         0.171287         0.658417          0.912277   \n",
       "30       5.226751         0.133294         0.705559          0.948703   \n",
       "31       6.393644         0.125001         0.737183          0.967178   \n",
       "32       7.703230         0.173632         0.773915          0.978883   \n",
       "33       9.351722         0.124458         0.792575          0.984954   \n",
       "34      10.496654         0.257629         0.804164          0.987552   \n",
       "35       3.783091         0.149218         0.532115          0.850935   \n",
       "36       5.747668         0.154155         0.646042          0.941060   \n",
       "37       5.749373         0.138602         0.705952          0.970017   \n",
       "38       6.762719         0.140030         0.738165          0.981350   \n",
       "39       8.987674         0.125300         0.778629          0.988994   \n",
       "40      10.700832         0.258806         0.797682          0.992444   \n",
       "41      13.170077         0.258905         0.807307          0.993470   \n",
       "\n",
       "   param_max_depth param_n_estimators                                 params  \\\n",
       "0               20                  5   {'n_estimators': 5, 'max_depth': 20}   \n",
       "1               20                 10  {'n_estimators': 10, 'max_depth': 20}   \n",
       "2               20                 15  {'n_estimators': 15, 'max_depth': 20}   \n",
       "3               20                 20  {'n_estimators': 20, 'max_depth': 20}   \n",
       "4               20                 30  {'n_estimators': 30, 'max_depth': 20}   \n",
       "5               20                 40  {'n_estimators': 40, 'max_depth': 20}   \n",
       "6               20                 50  {'n_estimators': 50, 'max_depth': 20}   \n",
       "7               30                  5   {'n_estimators': 5, 'max_depth': 30}   \n",
       "8               30                 10  {'n_estimators': 10, 'max_depth': 30}   \n",
       "9               30                 15  {'n_estimators': 15, 'max_depth': 30}   \n",
       "10              30                 20  {'n_estimators': 20, 'max_depth': 30}   \n",
       "11              30                 30  {'n_estimators': 30, 'max_depth': 30}   \n",
       "12              30                 40  {'n_estimators': 40, 'max_depth': 30}   \n",
       "13              30                 50  {'n_estimators': 50, 'max_depth': 30}   \n",
       "14              40                  5   {'n_estimators': 5, 'max_depth': 40}   \n",
       "15              40                 10  {'n_estimators': 10, 'max_depth': 40}   \n",
       "16              40                 15  {'n_estimators': 15, 'max_depth': 40}   \n",
       "17              40                 20  {'n_estimators': 20, 'max_depth': 40}   \n",
       "18              40                 30  {'n_estimators': 30, 'max_depth': 40}   \n",
       "19              40                 40  {'n_estimators': 40, 'max_depth': 40}   \n",
       "20              40                 50  {'n_estimators': 50, 'max_depth': 40}   \n",
       "21              50                  5   {'n_estimators': 5, 'max_depth': 50}   \n",
       "22              50                 10  {'n_estimators': 10, 'max_depth': 50}   \n",
       "23              50                 15  {'n_estimators': 15, 'max_depth': 50}   \n",
       "24              50                 20  {'n_estimators': 20, 'max_depth': 50}   \n",
       "25              50                 30  {'n_estimators': 30, 'max_depth': 50}   \n",
       "26              50                 40  {'n_estimators': 40, 'max_depth': 50}   \n",
       "27              50                 50  {'n_estimators': 50, 'max_depth': 50}   \n",
       "28              60                  5   {'n_estimators': 5, 'max_depth': 60}   \n",
       "29              60                 10  {'n_estimators': 10, 'max_depth': 60}   \n",
       "30              60                 15  {'n_estimators': 15, 'max_depth': 60}   \n",
       "31              60                 20  {'n_estimators': 20, 'max_depth': 60}   \n",
       "32              60                 30  {'n_estimators': 30, 'max_depth': 60}   \n",
       "33              60                 40  {'n_estimators': 40, 'max_depth': 60}   \n",
       "34              60                 50  {'n_estimators': 50, 'max_depth': 60}   \n",
       "35              70                  5   {'n_estimators': 5, 'max_depth': 70}   \n",
       "36              70                 10  {'n_estimators': 10, 'max_depth': 70}   \n",
       "37              70                 15  {'n_estimators': 15, 'max_depth': 70}   \n",
       "38              70                 20  {'n_estimators': 20, 'max_depth': 70}   \n",
       "39              70                 30  {'n_estimators': 30, 'max_depth': 70}   \n",
       "40              70                 40  {'n_estimators': 40, 'max_depth': 70}   \n",
       "41              70                 50  {'n_estimators': 50, 'max_depth': 70}   \n",
       "\n",
       "    rank_test_score  split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0                42           0.424278            0.494824           0.426635   \n",
       "1                39           0.503241            0.623755           0.532115   \n",
       "2                35           0.575722            0.700865           0.591632   \n",
       "3                32           0.629935            0.754782           0.635828   \n",
       "4                26           0.674131            0.797956           0.676488   \n",
       "5                20           0.705952            0.829468           0.715380   \n",
       "6                19           0.723630            0.844864           0.723041   \n",
       "7                41           0.469063            0.609342           0.467885   \n",
       "8                34           0.577490            0.746004           0.578079   \n",
       "9                30           0.626400            0.811386           0.654685   \n",
       "10               25           0.677077            0.849515           0.674131   \n",
       "11               17           0.727166            0.890592           0.719505   \n",
       "12               13           0.743665            0.910443           0.746022   \n",
       "13               11           0.761933            0.923611           0.748379   \n",
       "14               40           0.507955            0.706237           0.480259   \n",
       "15               33           0.610489            0.830189           0.604596   \n",
       "16               27           0.663524            0.878538           0.667649   \n",
       "17               23           0.693577            0.910705           0.693577   \n",
       "18               14           0.747201            0.933766           0.736005   \n",
       "19               10           0.773718            0.948244           0.758986   \n",
       "20                6           0.796111            0.957613           0.774308   \n",
       "21               38           0.535062            0.772209           0.525633   \n",
       "22               31           0.635239            0.880372           0.639364   \n",
       "23               24           0.684148            0.923415           0.698291   \n",
       "24               18           0.720094            0.946803           0.724808   \n",
       "25               12           0.758397            0.965671           0.753683   \n",
       "26                8           0.765468            0.974384           0.776665   \n",
       "27                4           0.789629            0.978774           0.787861   \n",
       "28               36           0.549794            0.830123           0.533883   \n",
       "29               28           0.653506            0.917191           0.667060   \n",
       "30               22           0.695345            0.952503           0.723041   \n",
       "31               16           0.728933            0.970846           0.741308   \n",
       "32                9           0.780790            0.980870           0.768415   \n",
       "33                5           0.802004            0.986832           0.786093   \n",
       "34                2           0.806718            0.988994           0.799057   \n",
       "35               37           0.532705            0.861701           0.532115   \n",
       "36               29           0.637006            0.944182           0.653506   \n",
       "37               21           0.692398            0.972681           0.715969   \n",
       "38               15           0.725987            0.983491           0.743076   \n",
       "39                7           0.775486            0.990959           0.772540   \n",
       "40                3           0.789629            0.993449           0.797879   \n",
       "41                1           0.800236            0.994693           0.806718   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0             0.495545           0.384207            0.472353      0.081700   \n",
       "1             0.634762           0.492045            0.618252      0.125276   \n",
       "2             0.705516           0.555097            0.696868      0.084782   \n",
       "3             0.751310           0.607543            0.747248      0.061035   \n",
       "4             0.810273           0.675309            0.811452      0.071779   \n",
       "5             0.839557           0.705952            0.832744      0.294505   \n",
       "6             0.854232           0.730112            0.852660      0.138987   \n",
       "7             0.599515           0.459635            0.600891      0.044893   \n",
       "8             0.749017           0.574543            0.748297      0.065387   \n",
       "9             0.812959           0.644667            0.820558      0.172783   \n",
       "10            0.850956           0.685916            0.856067      0.127854   \n",
       "11            0.892361           0.748379            0.897406      0.282430   \n",
       "12            0.910443           0.760754            0.913653      0.253957   \n",
       "13            0.918436           0.785504            0.925314      0.264881   \n",
       "14            0.687828           0.473777            0.684355      0.795735   \n",
       "15            0.820362           0.601650            0.818920      0.105300   \n",
       "16            0.880831           0.668238            0.877293      0.201872   \n",
       "17            0.912343           0.710666            0.907429      1.048325   \n",
       "18            0.939531           0.746612            0.936648      0.273186   \n",
       "19            0.950668           0.776665            0.949161      0.534332   \n",
       "20            0.957023           0.788450            0.958333      0.170681   \n",
       "21            0.755962           0.491456            0.750852      0.674180   \n",
       "22            0.874410           0.616382            0.877752      0.190985   \n",
       "23            0.923808           0.681791            0.926363      0.033240   \n",
       "24            0.945689           0.733648            0.946541      0.065602   \n",
       "25            0.963968           0.772540            0.964688      0.161436   \n",
       "26            0.972419           0.787861            0.975301      0.077515   \n",
       "27            0.977136           0.800825            0.979036      0.998968   \n",
       "28            0.798546           0.528580            0.803525      0.136971   \n",
       "29            0.904481           0.654685            0.915160      0.041833   \n",
       "30            0.943265           0.698291            0.950341      0.075408   \n",
       "31            0.964819           0.741308            0.965867      0.048445   \n",
       "32            0.977725           0.772540            0.978053      0.156272   \n",
       "33            0.982835           0.789629            0.985194      0.127502   \n",
       "34            0.985718           0.806718            0.987945      1.254174   \n",
       "35            0.841916           0.531526            0.849188      0.172407   \n",
       "36            0.935469           0.647613            0.943527      0.659774   \n",
       "37            0.967309           0.709487            0.970060      0.126266   \n",
       "38            0.979756           0.745433            0.980805      0.792023   \n",
       "39            0.989321           0.787861            0.986701      0.132610   \n",
       "40            0.992859           0.805539            0.991025      0.155441   \n",
       "41            0.993776           0.814968            0.991942      0.082882   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "0         0.006569        0.019469         0.010767  \n",
       "1         0.010700        0.016881         0.006864  \n",
       "2         0.008742        0.014957         0.003534  \n",
       "3         0.012219        0.012185         0.003079  \n",
       "4         0.001413        0.000962         0.006103  \n",
       "5         0.049824        0.004445         0.004202  \n",
       "6         0.003589        0.003204         0.004096  \n",
       "7         0.004418        0.004194         0.004345  \n",
       "8         0.009129        0.001547         0.001285  \n",
       "9         0.002748        0.011710         0.004005  \n",
       "10        0.004686        0.005008         0.002811  \n",
       "11        0.062495        0.012213         0.002887  \n",
       "12        0.063661        0.007562         0.001513  \n",
       "13        0.051579        0.015339         0.002925  \n",
       "14        0.020983        0.014822         0.009602  \n",
       "15        0.002239        0.003675         0.005007  \n",
       "16        0.002367        0.002097         0.001465  \n",
       "17        0.000893        0.008056         0.002043  \n",
       "18        0.000497        0.005145         0.002354  \n",
       "19        0.069268        0.007733         0.000999  \n",
       "20        0.060580        0.009031         0.000536  \n",
       "21        0.010698        0.018734         0.009106  \n",
       "22        0.003909        0.010004         0.002440  \n",
       "23        0.001188        0.007286         0.001307  \n",
       "24        0.004193        0.005618         0.000475  \n",
       "25        0.004547        0.008013         0.000698  \n",
       "26        0.064190        0.009142         0.001202  \n",
       "27        0.047472        0.005740         0.000841  \n",
       "28        0.012647        0.009014         0.013862  \n",
       "29        0.005858        0.006130         0.005575  \n",
       "30        0.002524        0.012420         0.003945  \n",
       "31        0.000636        0.005834         0.002629  \n",
       "32        0.072542        0.005145         0.001412  \n",
       "33        0.000459        0.006821         0.001640  \n",
       "34        0.018957        0.003611         0.001366  \n",
       "35        0.011259        0.000481         0.008171  \n",
       "36        0.020343        0.006827         0.003962  \n",
       "37        0.023520        0.009942         0.002193  \n",
       "38        0.023083        0.008665         0.001573  \n",
       "39        0.001527        0.006638         0.001754  \n",
       "40        0.019988        0.006497         0.001032  \n",
       "41        0.021130        0.006029         0.001144  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Observation for further experiment\n",
    "\n",
    "We notice that in the above experiment the best results are obtained for the highest value of parameters thus suggesting to try even higher values. Using a higher but narrower range this time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=0, test_size=0.1, train_size=None),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=8, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [40, 50, 60, 70, 80, 90], 'max_depth': [60, 70, 80, 90, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid2= {'max_depth': [60, 70, 80,90,100],\n",
    "              'n_estimators': [40, 50, 60,70,80,90]}\n",
    "clf2 = GridSearchCV(RandomForestClassifier(random_state=0 , n_jobs = 8), param_grid2 , cv = cv)\n",
    "%time clf2.fit(vectors_cv, labels_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results2 = pd.DataFrame(clf2.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.025961</td>\n",
       "      <td>0.213422</td>\n",
       "      <td>0.792575</td>\n",
       "      <td>0.984954</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 60}</td>\n",
       "      <td>30</td>\n",
       "      <td>0.802004</td>\n",
       "      <td>0.986832</td>\n",
       "      <td>0.786093</td>\n",
       "      <td>0.982835</td>\n",
       "      <td>0.789629</td>\n",
       "      <td>0.985194</td>\n",
       "      <td>0.599586</td>\n",
       "      <td>0.065213</td>\n",
       "      <td>0.006821</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.211836</td>\n",
       "      <td>0.281760</td>\n",
       "      <td>0.804164</td>\n",
       "      <td>0.987552</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 60}</td>\n",
       "      <td>27</td>\n",
       "      <td>0.806718</td>\n",
       "      <td>0.988994</td>\n",
       "      <td>0.799057</td>\n",
       "      <td>0.985718</td>\n",
       "      <td>0.806718</td>\n",
       "      <td>0.987945</td>\n",
       "      <td>0.268820</td>\n",
       "      <td>0.070085</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.001366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.134366</td>\n",
       "      <td>0.379295</td>\n",
       "      <td>0.812610</td>\n",
       "      <td>0.988753</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>{'n_estimators': 60, 'max_depth': 60}</td>\n",
       "      <td>21</td>\n",
       "      <td>0.814378</td>\n",
       "      <td>0.988994</td>\n",
       "      <td>0.809664</td>\n",
       "      <td>0.987618</td>\n",
       "      <td>0.813789</td>\n",
       "      <td>0.989649</td>\n",
       "      <td>0.494110</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.000846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.383046</td>\n",
       "      <td>0.319809</td>\n",
       "      <td>0.819093</td>\n",
       "      <td>0.989780</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "      <td>{'n_estimators': 70, 'max_depth': 60}</td>\n",
       "      <td>18</td>\n",
       "      <td>0.823217</td>\n",
       "      <td>0.990304</td>\n",
       "      <td>0.817914</td>\n",
       "      <td>0.988863</td>\n",
       "      <td>0.816146</td>\n",
       "      <td>0.990173</td>\n",
       "      <td>0.506953</td>\n",
       "      <td>0.063684</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.452258</td>\n",
       "      <td>0.319094</td>\n",
       "      <td>0.826164</td>\n",
       "      <td>0.990719</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 60}</td>\n",
       "      <td>14</td>\n",
       "      <td>0.827932</td>\n",
       "      <td>0.990632</td>\n",
       "      <td>0.822039</td>\n",
       "      <td>0.990042</td>\n",
       "      <td>0.828521</td>\n",
       "      <td>0.991483</td>\n",
       "      <td>0.430513</td>\n",
       "      <td>0.059957</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.000592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17.555641</td>\n",
       "      <td>0.315719</td>\n",
       "      <td>0.831664</td>\n",
       "      <td>0.991549</td>\n",
       "      <td>60</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 60}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.836181</td>\n",
       "      <td>0.992204</td>\n",
       "      <td>0.824985</td>\n",
       "      <td>0.990501</td>\n",
       "      <td>0.833824</td>\n",
       "      <td>0.991942</td>\n",
       "      <td>0.572023</td>\n",
       "      <td>0.053112</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>0.000749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11.490247</td>\n",
       "      <td>0.200874</td>\n",
       "      <td>0.797682</td>\n",
       "      <td>0.992444</td>\n",
       "      <td>70</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 70}</td>\n",
       "      <td>28</td>\n",
       "      <td>0.789629</td>\n",
       "      <td>0.993449</td>\n",
       "      <td>0.797879</td>\n",
       "      <td>0.992859</td>\n",
       "      <td>0.805539</td>\n",
       "      <td>0.991025</td>\n",
       "      <td>0.057396</td>\n",
       "      <td>0.053809</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>0.001032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.546562</td>\n",
       "      <td>0.249444</td>\n",
       "      <td>0.807307</td>\n",
       "      <td>0.993470</td>\n",
       "      <td>70</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 70}</td>\n",
       "      <td>24</td>\n",
       "      <td>0.800236</td>\n",
       "      <td>0.994693</td>\n",
       "      <td>0.806718</td>\n",
       "      <td>0.993776</td>\n",
       "      <td>0.814968</td>\n",
       "      <td>0.991942</td>\n",
       "      <td>0.243048</td>\n",
       "      <td>0.022790</td>\n",
       "      <td>0.006029</td>\n",
       "      <td>0.001144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17.747160</td>\n",
       "      <td>0.345111</td>\n",
       "      <td>0.818503</td>\n",
       "      <td>0.994082</td>\n",
       "      <td>70</td>\n",
       "      <td>60</td>\n",
       "      <td>{'n_estimators': 60, 'max_depth': 70}</td>\n",
       "      <td>19</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>0.995086</td>\n",
       "      <td>0.818503</td>\n",
       "      <td>0.994169</td>\n",
       "      <td>0.823807</td>\n",
       "      <td>0.992990</td>\n",
       "      <td>0.877132</td>\n",
       "      <td>0.073292</td>\n",
       "      <td>0.004330</td>\n",
       "      <td>0.000858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19.579997</td>\n",
       "      <td>0.281938</td>\n",
       "      <td>0.825378</td>\n",
       "      <td>0.994759</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>{'n_estimators': 70, 'max_depth': 70}</td>\n",
       "      <td>15</td>\n",
       "      <td>0.823217</td>\n",
       "      <td>0.995480</td>\n",
       "      <td>0.825575</td>\n",
       "      <td>0.994824</td>\n",
       "      <td>0.827342</td>\n",
       "      <td>0.993973</td>\n",
       "      <td>1.045243</td>\n",
       "      <td>0.069548</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.374186</td>\n",
       "      <td>0.367963</td>\n",
       "      <td>0.831074</td>\n",
       "      <td>0.995130</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 70}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.827342</td>\n",
       "      <td>0.995742</td>\n",
       "      <td>0.829699</td>\n",
       "      <td>0.994955</td>\n",
       "      <td>0.836181</td>\n",
       "      <td>0.994693</td>\n",
       "      <td>0.856798</td>\n",
       "      <td>0.096996</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.000445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22.895077</td>\n",
       "      <td>0.397684</td>\n",
       "      <td>0.838735</td>\n",
       "      <td>0.995305</td>\n",
       "      <td>70</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 70}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.832646</td>\n",
       "      <td>0.995545</td>\n",
       "      <td>0.837360</td>\n",
       "      <td>0.995349</td>\n",
       "      <td>0.846199</td>\n",
       "      <td>0.995021</td>\n",
       "      <td>0.477978</td>\n",
       "      <td>0.065537</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.347359</td>\n",
       "      <td>0.201149</td>\n",
       "      <td>0.796307</td>\n",
       "      <td>0.995763</td>\n",
       "      <td>80</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 80}</td>\n",
       "      <td>29</td>\n",
       "      <td>0.796700</td>\n",
       "      <td>0.996659</td>\n",
       "      <td>0.791397</td>\n",
       "      <td>0.995676</td>\n",
       "      <td>0.800825</td>\n",
       "      <td>0.994955</td>\n",
       "      <td>0.665664</td>\n",
       "      <td>0.055494</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>0.000698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.852067</td>\n",
       "      <td>0.233511</td>\n",
       "      <td>0.806521</td>\n",
       "      <td>0.996659</td>\n",
       "      <td>80</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 80}</td>\n",
       "      <td>25</td>\n",
       "      <td>0.807896</td>\n",
       "      <td>0.997117</td>\n",
       "      <td>0.800236</td>\n",
       "      <td>0.997183</td>\n",
       "      <td>0.811432</td>\n",
       "      <td>0.995676</td>\n",
       "      <td>0.387311</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.000695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17.686255</td>\n",
       "      <td>0.341814</td>\n",
       "      <td>0.814771</td>\n",
       "      <td>0.997074</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>{'n_estimators': 60, 'max_depth': 80}</td>\n",
       "      <td>20</td>\n",
       "      <td>0.817325</td>\n",
       "      <td>0.997248</td>\n",
       "      <td>0.805539</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>0.821450</td>\n",
       "      <td>0.996593</td>\n",
       "      <td>0.647795</td>\n",
       "      <td>0.075962</td>\n",
       "      <td>0.006742</td>\n",
       "      <td>0.000344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20.224815</td>\n",
       "      <td>0.376749</td>\n",
       "      <td>0.827539</td>\n",
       "      <td>0.997445</td>\n",
       "      <td>80</td>\n",
       "      <td>70</td>\n",
       "      <td>{'n_estimators': 70, 'max_depth': 80}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.829110</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>0.820860</td>\n",
       "      <td>0.997642</td>\n",
       "      <td>0.832646</td>\n",
       "      <td>0.997314</td>\n",
       "      <td>0.438943</td>\n",
       "      <td>0.019710</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22.620149</td>\n",
       "      <td>0.345794</td>\n",
       "      <td>0.833039</td>\n",
       "      <td>0.997576</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 80}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.833235</td>\n",
       "      <td>0.997248</td>\n",
       "      <td>0.826164</td>\n",
       "      <td>0.997707</td>\n",
       "      <td>0.839717</td>\n",
       "      <td>0.997773</td>\n",
       "      <td>0.771631</td>\n",
       "      <td>0.010366</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.000233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25.049169</td>\n",
       "      <td>0.434645</td>\n",
       "      <td>0.835985</td>\n",
       "      <td>0.997860</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 80}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.833824</td>\n",
       "      <td>0.997510</td>\n",
       "      <td>0.832646</td>\n",
       "      <td>0.998035</td>\n",
       "      <td>0.841485</td>\n",
       "      <td>0.998035</td>\n",
       "      <td>1.094069</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.114398</td>\n",
       "      <td>0.177085</td>\n",
       "      <td>0.808289</td>\n",
       "      <td>0.997598</td>\n",
       "      <td>90</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 90}</td>\n",
       "      <td>23</td>\n",
       "      <td>0.804361</td>\n",
       "      <td>0.998362</td>\n",
       "      <td>0.810253</td>\n",
       "      <td>0.997445</td>\n",
       "      <td>0.810253</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.561295</td>\n",
       "      <td>0.073692</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.000572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16.507921</td>\n",
       "      <td>0.263284</td>\n",
       "      <td>0.812021</td>\n",
       "      <td>0.998275</td>\n",
       "      <td>90</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 90}</td>\n",
       "      <td>22</td>\n",
       "      <td>0.819682</td>\n",
       "      <td>0.998493</td>\n",
       "      <td>0.802593</td>\n",
       "      <td>0.998297</td>\n",
       "      <td>0.813789</td>\n",
       "      <td>0.998035</td>\n",
       "      <td>0.522164</td>\n",
       "      <td>0.020623</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.599260</td>\n",
       "      <td>0.272431</td>\n",
       "      <td>0.821842</td>\n",
       "      <td>0.998340</td>\n",
       "      <td>90</td>\n",
       "      <td>60</td>\n",
       "      <td>{'n_estimators': 60, 'max_depth': 90}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.822039</td>\n",
       "      <td>0.998559</td>\n",
       "      <td>0.814968</td>\n",
       "      <td>0.998297</td>\n",
       "      <td>0.828521</td>\n",
       "      <td>0.998166</td>\n",
       "      <td>0.810429</td>\n",
       "      <td>0.050471</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23.123424</td>\n",
       "      <td>0.282924</td>\n",
       "      <td>0.829110</td>\n",
       "      <td>0.998755</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>{'n_estimators': 70, 'max_depth': 90}</td>\n",
       "      <td>12</td>\n",
       "      <td>0.829110</td>\n",
       "      <td>0.998559</td>\n",
       "      <td>0.821450</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>0.836771</td>\n",
       "      <td>0.999017</td>\n",
       "      <td>0.831682</td>\n",
       "      <td>0.066524</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25.080649</td>\n",
       "      <td>0.444063</td>\n",
       "      <td>0.833039</td>\n",
       "      <td>0.998733</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 90}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.833235</td>\n",
       "      <td>0.998559</td>\n",
       "      <td>0.826164</td>\n",
       "      <td>0.998755</td>\n",
       "      <td>0.839717</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>0.404183</td>\n",
       "      <td>0.070223</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>27.615274</td>\n",
       "      <td>0.408536</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.998843</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 90}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.838539</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>0.823807</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>0.840896</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>0.432787</td>\n",
       "      <td>0.121449</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17.132351</td>\n",
       "      <td>0.253354</td>\n",
       "      <td>0.805932</td>\n",
       "      <td>0.998843</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 100}</td>\n",
       "      <td>26</td>\n",
       "      <td>0.806128</td>\n",
       "      <td>0.999083</td>\n",
       "      <td>0.799057</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>0.812610</td>\n",
       "      <td>0.998493</td>\n",
       "      <td>0.469832</td>\n",
       "      <td>0.018623</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18.696790</td>\n",
       "      <td>0.290734</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 100}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.829699</td>\n",
       "      <td>0.999279</td>\n",
       "      <td>0.816735</td>\n",
       "      <td>0.999279</td>\n",
       "      <td>0.826164</td>\n",
       "      <td>0.998821</td>\n",
       "      <td>0.965493</td>\n",
       "      <td>0.061541</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21.304539</td>\n",
       "      <td>0.323326</td>\n",
       "      <td>0.829307</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>{'n_estimators': 60, 'max_depth': 100}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.835003</td>\n",
       "      <td>0.999279</td>\n",
       "      <td>0.819682</td>\n",
       "      <td>0.999148</td>\n",
       "      <td>0.833235</td>\n",
       "      <td>0.999279</td>\n",
       "      <td>0.649121</td>\n",
       "      <td>0.059317</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>24.833774</td>\n",
       "      <td>0.380186</td>\n",
       "      <td>0.832449</td>\n",
       "      <td>0.999258</td>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "      <td>{'n_estimators': 70, 'max_depth': 100}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>0.827932</td>\n",
       "      <td>0.999017</td>\n",
       "      <td>0.835003</td>\n",
       "      <td>0.999410</td>\n",
       "      <td>0.490927</td>\n",
       "      <td>0.014730</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28.503517</td>\n",
       "      <td>0.305826</td>\n",
       "      <td>0.835985</td>\n",
       "      <td>0.999301</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 100}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.840306</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>0.828521</td>\n",
       "      <td>0.999017</td>\n",
       "      <td>0.839128</td>\n",
       "      <td>0.999541</td>\n",
       "      <td>2.251026</td>\n",
       "      <td>0.052562</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.801429</td>\n",
       "      <td>0.401652</td>\n",
       "      <td>0.843449</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 100}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999214</td>\n",
       "      <td>0.838539</td>\n",
       "      <td>0.999214</td>\n",
       "      <td>0.846199</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>1.477846</td>\n",
       "      <td>0.115026</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       10.025961         0.213422         0.792575          0.984954   \n",
       "1       11.211836         0.281760         0.804164          0.987552   \n",
       "2       13.134366         0.379295         0.812610          0.988753   \n",
       "3       15.383046         0.319809         0.819093          0.989780   \n",
       "4       17.452258         0.319094         0.826164          0.990719   \n",
       "5       17.555641         0.315719         0.831664          0.991549   \n",
       "6       11.490247         0.200874         0.797682          0.992444   \n",
       "7       13.546562         0.249444         0.807307          0.993470   \n",
       "8       17.747160         0.345111         0.818503          0.994082   \n",
       "9       19.579997         0.281938         0.825378          0.994759   \n",
       "10      20.374186         0.367963         0.831074          0.995130   \n",
       "11      22.895077         0.397684         0.838735          0.995305   \n",
       "12      13.347359         0.201149         0.796307          0.995763   \n",
       "13      15.852067         0.233511         0.806521          0.996659   \n",
       "14      17.686255         0.341814         0.814771          0.997074   \n",
       "15      20.224815         0.376749         0.827539          0.997445   \n",
       "16      22.620149         0.345794         0.833039          0.997576   \n",
       "17      25.049169         0.434645         0.835985          0.997860   \n",
       "18      14.114398         0.177085         0.808289          0.997598   \n",
       "19      16.507921         0.263284         0.812021          0.998275   \n",
       "20      20.599260         0.272431         0.821842          0.998340   \n",
       "21      23.123424         0.282924         0.829110          0.998755   \n",
       "22      25.080649         0.444063         0.833039          0.998733   \n",
       "23      27.615274         0.408536         0.834414          0.998843   \n",
       "24      17.132351         0.253354         0.805932          0.998843   \n",
       "25      18.696790         0.290734         0.824200          0.999126   \n",
       "26      21.304539         0.323326         0.829307          0.999236   \n",
       "27      24.833774         0.380186         0.832449          0.999258   \n",
       "28      28.503517         0.305826         0.835985          0.999301   \n",
       "29      30.801429         0.401652         0.843449          0.999345   \n",
       "\n",
       "   param_max_depth param_n_estimators                                  params  \\\n",
       "0               60                 40   {'n_estimators': 40, 'max_depth': 60}   \n",
       "1               60                 50   {'n_estimators': 50, 'max_depth': 60}   \n",
       "2               60                 60   {'n_estimators': 60, 'max_depth': 60}   \n",
       "3               60                 70   {'n_estimators': 70, 'max_depth': 60}   \n",
       "4               60                 80   {'n_estimators': 80, 'max_depth': 60}   \n",
       "5               60                 90   {'n_estimators': 90, 'max_depth': 60}   \n",
       "6               70                 40   {'n_estimators': 40, 'max_depth': 70}   \n",
       "7               70                 50   {'n_estimators': 50, 'max_depth': 70}   \n",
       "8               70                 60   {'n_estimators': 60, 'max_depth': 70}   \n",
       "9               70                 70   {'n_estimators': 70, 'max_depth': 70}   \n",
       "10              70                 80   {'n_estimators': 80, 'max_depth': 70}   \n",
       "11              70                 90   {'n_estimators': 90, 'max_depth': 70}   \n",
       "12              80                 40   {'n_estimators': 40, 'max_depth': 80}   \n",
       "13              80                 50   {'n_estimators': 50, 'max_depth': 80}   \n",
       "14              80                 60   {'n_estimators': 60, 'max_depth': 80}   \n",
       "15              80                 70   {'n_estimators': 70, 'max_depth': 80}   \n",
       "16              80                 80   {'n_estimators': 80, 'max_depth': 80}   \n",
       "17              80                 90   {'n_estimators': 90, 'max_depth': 80}   \n",
       "18              90                 40   {'n_estimators': 40, 'max_depth': 90}   \n",
       "19              90                 50   {'n_estimators': 50, 'max_depth': 90}   \n",
       "20              90                 60   {'n_estimators': 60, 'max_depth': 90}   \n",
       "21              90                 70   {'n_estimators': 70, 'max_depth': 90}   \n",
       "22              90                 80   {'n_estimators': 80, 'max_depth': 90}   \n",
       "23              90                 90   {'n_estimators': 90, 'max_depth': 90}   \n",
       "24             100                 40  {'n_estimators': 40, 'max_depth': 100}   \n",
       "25             100                 50  {'n_estimators': 50, 'max_depth': 100}   \n",
       "26             100                 60  {'n_estimators': 60, 'max_depth': 100}   \n",
       "27             100                 70  {'n_estimators': 70, 'max_depth': 100}   \n",
       "28             100                 80  {'n_estimators': 80, 'max_depth': 100}   \n",
       "29             100                 90  {'n_estimators': 90, 'max_depth': 100}   \n",
       "\n",
       "    rank_test_score  split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0                30           0.802004            0.986832           0.786093   \n",
       "1                27           0.806718            0.988994           0.799057   \n",
       "2                21           0.814378            0.988994           0.809664   \n",
       "3                18           0.823217            0.990304           0.817914   \n",
       "4                14           0.827932            0.990632           0.822039   \n",
       "5                 9           0.836181            0.992204           0.824985   \n",
       "6                28           0.789629            0.993449           0.797879   \n",
       "7                24           0.800236            0.994693           0.806718   \n",
       "8                19           0.813200            0.995086           0.818503   \n",
       "9                15           0.823217            0.995480           0.825575   \n",
       "10               10           0.827342            0.995742           0.829699   \n",
       "11                2           0.832646            0.995545           0.837360   \n",
       "12               29           0.796700            0.996659           0.791397   \n",
       "13               25           0.807896            0.997117           0.800236   \n",
       "14               20           0.817325            0.997248           0.805539   \n",
       "15               13           0.829110            0.997379           0.820860   \n",
       "16                6           0.833235            0.997248           0.826164   \n",
       "17                3           0.833824            0.997510           0.832646   \n",
       "18               23           0.804361            0.998362           0.810253   \n",
       "19               22           0.819682            0.998493           0.802593   \n",
       "20               17           0.822039            0.998559           0.814968   \n",
       "21               12           0.829110            0.998559           0.821450   \n",
       "22                6           0.833235            0.998559           0.826164   \n",
       "23                5           0.838539            0.998690           0.823807   \n",
       "24               26           0.806128            0.999083           0.799057   \n",
       "25               16           0.829699            0.999279           0.816735   \n",
       "26               11           0.835003            0.999279           0.819682   \n",
       "27                8           0.834414            0.999345           0.827932   \n",
       "28                3           0.840306            0.999345           0.828521   \n",
       "29                1           0.845610            0.999214           0.838539   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0             0.982835           0.789629            0.985194      0.599586   \n",
       "1             0.985718           0.806718            0.987945      0.268820   \n",
       "2             0.987618           0.813789            0.989649      0.494110   \n",
       "3             0.988863           0.816146            0.990173      0.506953   \n",
       "4             0.990042           0.828521            0.991483      0.430513   \n",
       "5             0.990501           0.833824            0.991942      0.572023   \n",
       "6             0.992859           0.805539            0.991025      0.057396   \n",
       "7             0.993776           0.814968            0.991942      0.243048   \n",
       "8             0.994169           0.823807            0.992990      0.877132   \n",
       "9             0.994824           0.827342            0.993973      1.045243   \n",
       "10            0.994955           0.836181            0.994693      0.856798   \n",
       "11            0.995349           0.846199            0.995021      0.477978   \n",
       "12            0.995676           0.800825            0.994955      0.665664   \n",
       "13            0.997183           0.811432            0.995676      0.387311   \n",
       "14            0.997379           0.821450            0.996593      0.647795   \n",
       "15            0.997642           0.832646            0.997314      0.438943   \n",
       "16            0.997707           0.839717            0.997773      0.771631   \n",
       "17            0.998035           0.841485            0.998035      1.094069   \n",
       "18            0.997445           0.810253            0.996986      0.561295   \n",
       "19            0.998297           0.813789            0.998035      0.522164   \n",
       "20            0.998297           0.828521            0.998166      0.810429   \n",
       "21            0.998690           0.836771            0.999017      0.831682   \n",
       "22            0.998755           0.839717            0.998886      0.404183   \n",
       "23            0.998886           0.840896            0.998952      0.432787   \n",
       "24            0.998952           0.812610            0.998493      0.469832   \n",
       "25            0.999279           0.826164            0.998821      0.965493   \n",
       "26            0.999148           0.833235            0.999279      0.649121   \n",
       "27            0.999017           0.835003            0.999410      0.490927   \n",
       "28            0.999017           0.839128            0.999541      2.251026   \n",
       "29            0.999214           0.846199            0.999607      1.477846   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "0         0.065213        0.006821         0.001640  \n",
       "1         0.070085        0.003611         0.001366  \n",
       "2         0.003539        0.002097         0.000846  \n",
       "3         0.063684        0.003005         0.000651  \n",
       "4         0.059957        0.002927         0.000592  \n",
       "5         0.053112        0.004819         0.000749  \n",
       "6         0.053809        0.006497         0.001032  \n",
       "7         0.022790        0.006029         0.001144  \n",
       "8         0.073292        0.004330         0.000858  \n",
       "9         0.069548        0.001690         0.000617  \n",
       "10        0.096996        0.003737         0.000445  \n",
       "11        0.065537        0.005618         0.000216  \n",
       "12        0.055494        0.003859         0.000698  \n",
       "13        0.002797        0.004673         0.000695  \n",
       "14        0.075962        0.006742         0.000344  \n",
       "15        0.019710        0.004938         0.000142  \n",
       "16        0.010366        0.005535         0.000233  \n",
       "17        0.059524        0.003919         0.000247  \n",
       "18        0.073692        0.002778         0.000572  \n",
       "19        0.020623        0.007088         0.000188  \n",
       "20        0.050471        0.005535         0.000163  \n",
       "21        0.066524        0.006255         0.000193  \n",
       "22        0.070223        0.005535         0.000135  \n",
       "23        0.121449        0.007562         0.000111  \n",
       "24        0.018623        0.005535         0.000253  \n",
       "25        0.061541        0.005472         0.000216  \n",
       "26        0.059317        0.006844         0.000062  \n",
       "27        0.014730        0.003204         0.000172  \n",
       "28        0.052562        0.005300         0.000216  \n",
       "29        0.115026        0.003481         0.000185  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 : Even further experiment\n",
    "\n",
    "The same problem repeats again. Using even higher values but with small range now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=0, test_size=0.1, train_size=None),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=8, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [80, 90, 100, 110], 'max_depth': [90, 100, 110, 120]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid3= {'max_depth': [90,100, 110, 120],\n",
    "              'n_estimators': [80,90, 100 , 110]}\n",
    "clf3 = GridSearchCV(RandomForestClassifier(random_state=0 , n_jobs = 8), param_grid3 , cv = cv)\n",
    "%time clf3.fit(vectors_cv, labels_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.795837</td>\n",
       "      <td>0.411968</td>\n",
       "      <td>0.833039</td>\n",
       "      <td>0.998733</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 90}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.833235</td>\n",
       "      <td>0.998559</td>\n",
       "      <td>0.826164</td>\n",
       "      <td>0.998755</td>\n",
       "      <td>0.839717</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>0.518043</td>\n",
       "      <td>0.049680</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>1.346177e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.082280</td>\n",
       "      <td>0.373256</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.998843</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 90}</td>\n",
       "      <td>14</td>\n",
       "      <td>0.838539</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>0.823807</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>0.840896</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>1.451036</td>\n",
       "      <td>0.141791</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>1.113518e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.292976</td>\n",
       "      <td>0.373348</td>\n",
       "      <td>0.836574</td>\n",
       "      <td>0.998843</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 90}</td>\n",
       "      <td>12</td>\n",
       "      <td>0.836181</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>0.830289</td>\n",
       "      <td>0.998821</td>\n",
       "      <td>0.843253</td>\n",
       "      <td>0.998821</td>\n",
       "      <td>1.037137</td>\n",
       "      <td>0.053993</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.470957</td>\n",
       "      <td>0.402448</td>\n",
       "      <td>0.839128</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>90</td>\n",
       "      <td>110</td>\n",
       "      <td>{'n_estimators': 110, 'max_depth': 90}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.842074</td>\n",
       "      <td>0.999017</td>\n",
       "      <td>0.830289</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>0.845021</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>0.270371</td>\n",
       "      <td>0.101395</td>\n",
       "      <td>0.006365</td>\n",
       "      <td>5.349165e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.376795</td>\n",
       "      <td>0.362958</td>\n",
       "      <td>0.835985</td>\n",
       "      <td>0.999301</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 100}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.840306</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>0.828521</td>\n",
       "      <td>0.999017</td>\n",
       "      <td>0.839128</td>\n",
       "      <td>0.999541</td>\n",
       "      <td>1.018903</td>\n",
       "      <td>0.107333</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>2.161839e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28.527824</td>\n",
       "      <td>0.297910</td>\n",
       "      <td>0.843449</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 100}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999214</td>\n",
       "      <td>0.838539</td>\n",
       "      <td>0.999214</td>\n",
       "      <td>0.846199</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>0.643824</td>\n",
       "      <td>0.050777</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>1.853005e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31.149265</td>\n",
       "      <td>0.459654</td>\n",
       "      <td>0.846003</td>\n",
       "      <td>0.999323</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 100}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.849735</td>\n",
       "      <td>0.999214</td>\n",
       "      <td>0.839717</td>\n",
       "      <td>0.999148</td>\n",
       "      <td>0.848556</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>0.670212</td>\n",
       "      <td>0.113889</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>2.025161e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>38.339701</td>\n",
       "      <td>0.515438</td>\n",
       "      <td>0.848753</td>\n",
       "      <td>0.999476</td>\n",
       "      <td>100</td>\n",
       "      <td>110</td>\n",
       "      <td>{'n_estimators': 110, 'max_depth': 100}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.849735</td>\n",
       "      <td>0.999410</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999279</td>\n",
       "      <td>0.850913</td>\n",
       "      <td>0.999738</td>\n",
       "      <td>1.447922</td>\n",
       "      <td>0.129968</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>1.928669e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30.370284</td>\n",
       "      <td>0.348293</td>\n",
       "      <td>0.834021</td>\n",
       "      <td>0.999716</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 110}</td>\n",
       "      <td>15</td>\n",
       "      <td>0.828521</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.839128</td>\n",
       "      <td>0.999803</td>\n",
       "      <td>0.400752</td>\n",
       "      <td>0.106480</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>6.176684e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32.558777</td>\n",
       "      <td>0.388287</td>\n",
       "      <td>0.838342</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>110</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 110}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.837949</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.842664</td>\n",
       "      <td>0.999738</td>\n",
       "      <td>1.728720</td>\n",
       "      <td>0.071644</td>\n",
       "      <td>0.003379</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>36.614595</td>\n",
       "      <td>0.398460</td>\n",
       "      <td>0.842664</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>110</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 110}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.840896</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.839128</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>0.847967</td>\n",
       "      <td>0.999803</td>\n",
       "      <td>0.765892</td>\n",
       "      <td>0.091244</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>8.170985e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>42.639916</td>\n",
       "      <td>0.448275</td>\n",
       "      <td>0.844038</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>{'n_estimators': 110, 'max_depth': 110}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.839128</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>0.847378</td>\n",
       "      <td>0.999803</td>\n",
       "      <td>1.281271</td>\n",
       "      <td>0.089096</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>8.170985e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>30.400612</td>\n",
       "      <td>0.479428</td>\n",
       "      <td>0.841092</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 120}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.844431</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.835003</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.843842</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.876327</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.004312</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33.295857</td>\n",
       "      <td>0.396021</td>\n",
       "      <td>0.841485</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 120}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.844431</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.622665</td>\n",
       "      <td>0.119412</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>35.765307</td>\n",
       "      <td>0.462118</td>\n",
       "      <td>0.844824</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 120}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.836771</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.852092</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.660004</td>\n",
       "      <td>0.104951</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>38.507540</td>\n",
       "      <td>0.539555</td>\n",
       "      <td>0.849146</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>120</td>\n",
       "      <td>110</td>\n",
       "      <td>{'n_estimators': 110, 'max_depth': 120}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.853270</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.841485</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>1.202296</td>\n",
       "      <td>0.071258</td>\n",
       "      <td>0.005422</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       24.795837         0.411968         0.833039          0.998733   \n",
       "1       28.082280         0.373256         0.834414          0.998843   \n",
       "2       30.292976         0.373348         0.836574          0.998843   \n",
       "3       30.470957         0.402448         0.839128          0.998952   \n",
       "4       25.376795         0.362958         0.835985          0.999301   \n",
       "5       28.527824         0.297910         0.843449          0.999345   \n",
       "6       31.149265         0.459654         0.846003          0.999323   \n",
       "7       38.339701         0.515438         0.848753          0.999476   \n",
       "8       30.370284         0.348293         0.834021          0.999716   \n",
       "9       32.558777         0.388287         0.838342          0.999694   \n",
       "10      36.614595         0.398460         0.842664          0.999694   \n",
       "11      42.639916         0.448275         0.844038          0.999694   \n",
       "12      30.400612         0.479428         0.841092          0.999891   \n",
       "13      33.295857         0.396021         0.841485          0.999913   \n",
       "14      35.765307         0.462118         0.844824          0.999913   \n",
       "15      38.507540         0.539555         0.849146          0.999934   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "0               90                 80   \n",
       "1               90                 90   \n",
       "2               90                100   \n",
       "3               90                110   \n",
       "4              100                 80   \n",
       "5              100                 90   \n",
       "6              100                100   \n",
       "7              100                110   \n",
       "8              110                 80   \n",
       "9              110                 90   \n",
       "10             110                100   \n",
       "11             110                110   \n",
       "12             120                 80   \n",
       "13             120                 90   \n",
       "14             120                100   \n",
       "15             120                110   \n",
       "\n",
       "                                     params  rank_test_score  \\\n",
       "0     {'n_estimators': 80, 'max_depth': 90}               16   \n",
       "1     {'n_estimators': 90, 'max_depth': 90}               14   \n",
       "2    {'n_estimators': 100, 'max_depth': 90}               12   \n",
       "3    {'n_estimators': 110, 'max_depth': 90}               10   \n",
       "4    {'n_estimators': 80, 'max_depth': 100}               13   \n",
       "5    {'n_estimators': 90, 'max_depth': 100}                6   \n",
       "6   {'n_estimators': 100, 'max_depth': 100}                3   \n",
       "7   {'n_estimators': 110, 'max_depth': 100}                2   \n",
       "8    {'n_estimators': 80, 'max_depth': 110}               15   \n",
       "9    {'n_estimators': 90, 'max_depth': 110}               11   \n",
       "10  {'n_estimators': 100, 'max_depth': 110}                7   \n",
       "11  {'n_estimators': 110, 'max_depth': 110}                5   \n",
       "12   {'n_estimators': 80, 'max_depth': 120}                9   \n",
       "13   {'n_estimators': 90, 'max_depth': 120}                8   \n",
       "14  {'n_estimators': 100, 'max_depth': 120}                4   \n",
       "15  {'n_estimators': 110, 'max_depth': 120}                1   \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0            0.833235            0.998559           0.826164   \n",
       "1            0.838539            0.998690           0.823807   \n",
       "2            0.836181            0.998886           0.830289   \n",
       "3            0.842074            0.999017           0.830289   \n",
       "4            0.840306            0.999345           0.828521   \n",
       "5            0.845610            0.999214           0.838539   \n",
       "6            0.849735            0.999214           0.839717   \n",
       "7            0.849735            0.999410           0.845610   \n",
       "8            0.828521            0.999672           0.834414   \n",
       "9            0.834414            0.999672           0.837949   \n",
       "10           0.840896            0.999672           0.839128   \n",
       "11           0.845610            0.999672           0.839128   \n",
       "12           0.844431            0.999934           0.835003   \n",
       "13           0.845610            0.999934           0.834414   \n",
       "14           0.845610            0.999934           0.836771   \n",
       "15           0.853270            0.999934           0.841485   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0             0.998755           0.839717            0.998886      0.518043   \n",
       "1             0.998886           0.840896            0.998952      1.451036   \n",
       "2             0.998821           0.843253            0.998821      1.037137   \n",
       "3             0.998886           0.845021            0.998952      0.270371   \n",
       "4             0.999017           0.839128            0.999541      1.018903   \n",
       "5             0.999214           0.846199            0.999607      0.643824   \n",
       "6             0.999148           0.848556            0.999607      0.670212   \n",
       "7             0.999279           0.850913            0.999738      1.447922   \n",
       "8             0.999672           0.839128            0.999803      0.400752   \n",
       "9             0.999672           0.842664            0.999738      1.728720   \n",
       "10            0.999607           0.847967            0.999803      0.765892   \n",
       "11            0.999607           0.847378            0.999803      1.281271   \n",
       "12            0.999869           0.843842            0.999869      0.876327   \n",
       "13            0.999934           0.844431            0.999869      0.622665   \n",
       "14            0.999934           0.852092            0.999869      0.660004   \n",
       "15            0.999934           0.852681            0.999934      1.202296   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "0         0.049680        0.005535     1.346177e-04  \n",
       "1         0.141791        0.007562     1.113518e-04  \n",
       "2         0.053993        0.005300     3.088342e-05  \n",
       "3         0.101395        0.006365     5.349165e-05  \n",
       "4         0.107333        0.005300     2.161839e-04  \n",
       "5         0.050777        0.003481     1.853005e-04  \n",
       "6         0.113889        0.004471     2.025161e-04  \n",
       "7         0.129968        0.002274     1.928669e-04  \n",
       "8         0.106480        0.004339     6.176684e-05  \n",
       "9         0.071644        0.003379     3.088342e-05  \n",
       "10        0.091244        0.003819     8.170985e-05  \n",
       "11        0.089096        0.003547     8.170985e-05  \n",
       "12        0.002250        0.004312     3.088342e-05  \n",
       "13        0.119412        0.005023     3.088342e-05  \n",
       "14        0.104951        0.006279     3.088342e-05  \n",
       "15        0.071258        0.005422     1.110223e-16  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results3 = pd.DataFrame(clf3.cv_results_)\n",
    "results3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 : The final step\n",
    "\n",
    "The same problem still repeats but since the gap in best estimators is much smaller now, this means this step should \n",
    "find the optimal parameters now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=0, test_size=0.1, train_size=None),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=8, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [100, 120, 140], 'max_depth': [120, 140, 160]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid4= {'max_depth': [120,  140, 160],\n",
    "              'n_estimators': [100 ,120, 140]}\n",
    "clf4 = GridSearchCV(RandomForestClassifier(random_state=0 , n_jobs = 8), param_grid4 , cv = cv)\n",
    "%time clf4.fit(vectors_cv, labels_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.849772</td>\n",
       "      <td>0.458792</td>\n",
       "      <td>0.844824</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 120}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.836771</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.852092</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.495320</td>\n",
       "      <td>0.178278</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43.615133</td>\n",
       "      <td>0.452934</td>\n",
       "      <td>0.849735</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>{'n_estimators': 120, 'max_depth': 120}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.855038</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.839717</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.854449</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.231218</td>\n",
       "      <td>0.106040</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.590655</td>\n",
       "      <td>0.535677</td>\n",
       "      <td>0.851306</td>\n",
       "      <td>0.999847</td>\n",
       "      <td>120</td>\n",
       "      <td>140</td>\n",
       "      <td>{'n_estimators': 140, 'max_depth': 120}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.856806</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.843253</td>\n",
       "      <td>0.999803</td>\n",
       "      <td>0.853860</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.553527</td>\n",
       "      <td>0.153332</td>\n",
       "      <td>0.005820</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.536621</td>\n",
       "      <td>0.467547</td>\n",
       "      <td>0.848949</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>140</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 140}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.852092</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.846788</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.847967</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.390505</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.254158</td>\n",
       "      <td>0.654150</td>\n",
       "      <td>0.851699</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>140</td>\n",
       "      <td>120</td>\n",
       "      <td>{'n_estimators': 120, 'max_depth': 140}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857395</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.848556</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.849146</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>1.298463</td>\n",
       "      <td>0.244252</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57.783405</td>\n",
       "      <td>0.574911</td>\n",
       "      <td>0.855431</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>{'n_estimators': 140, 'max_depth': 140}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.855628</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.853270</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.857395</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>1.499260</td>\n",
       "      <td>0.167357</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41.718227</td>\n",
       "      <td>0.460633</td>\n",
       "      <td>0.846003</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>160</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 160}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.848556</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.843253</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.846199</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.664722</td>\n",
       "      <td>0.061454</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56.259005</td>\n",
       "      <td>0.750183</td>\n",
       "      <td>0.850913</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>160</td>\n",
       "      <td>120</td>\n",
       "      <td>{'n_estimators': 120, 'max_depth': 160}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.846788</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.853270</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>2.127975</td>\n",
       "      <td>0.042717</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62.502033</td>\n",
       "      <td>0.593865</td>\n",
       "      <td>0.851896</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>160</td>\n",
       "      <td>140</td>\n",
       "      <td>{'n_estimators': 140, 'max_depth': 160}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.857395</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>4.680417</td>\n",
       "      <td>0.111873</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0      36.849772         0.458792         0.844824          0.999913   \n",
       "1      43.615133         0.452934         0.849735          0.999869   \n",
       "2      48.590655         0.535677         0.851306          0.999847   \n",
       "3      39.536621         0.467547         0.848949          0.999934   \n",
       "4      51.254158         0.654150         0.851699          0.999934   \n",
       "5      57.783405         0.574911         0.855431          0.999934   \n",
       "6      41.718227         0.460633         0.846003          0.999934   \n",
       "7      56.259005         0.750183         0.850913          0.999934   \n",
       "8      62.502033         0.593865         0.851896          0.999934   \n",
       "\n",
       "  param_max_depth param_n_estimators                                   params  \\\n",
       "0             120                100  {'n_estimators': 100, 'max_depth': 120}   \n",
       "1             120                120  {'n_estimators': 120, 'max_depth': 120}   \n",
       "2             120                140  {'n_estimators': 140, 'max_depth': 120}   \n",
       "3             140                100  {'n_estimators': 100, 'max_depth': 140}   \n",
       "4             140                120  {'n_estimators': 120, 'max_depth': 140}   \n",
       "5             140                140  {'n_estimators': 140, 'max_depth': 140}   \n",
       "6             160                100  {'n_estimators': 100, 'max_depth': 160}   \n",
       "7             160                120  {'n_estimators': 120, 'max_depth': 160}   \n",
       "8             160                140  {'n_estimators': 140, 'max_depth': 160}   \n",
       "\n",
       "   rank_test_score  split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0                9           0.845610            0.999934           0.836771   \n",
       "1                6           0.855038            0.999869           0.839717   \n",
       "2                4           0.856806            0.999869           0.843253   \n",
       "3                7           0.852092            0.999934           0.846788   \n",
       "4                3           0.857395            0.999934           0.848556   \n",
       "5                1           0.855628            0.999934           0.853270   \n",
       "6                8           0.848556            0.999934           0.843253   \n",
       "7                5           0.852681            0.999934           0.846788   \n",
       "8                2           0.852681            0.999934           0.845610   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.999934           0.852092            0.999869      0.495320   \n",
       "1            0.999869           0.854449            0.999869      0.231218   \n",
       "2            0.999803           0.853860            0.999869      0.553527   \n",
       "3            0.999934           0.847967            0.999934      0.390505   \n",
       "4            0.999934           0.849146            0.999934      1.298463   \n",
       "5            0.999934           0.857395            0.999934      1.499260   \n",
       "6            0.999934           0.846199            0.999934      0.664722   \n",
       "7            0.999934           0.853270            0.999934      2.127975   \n",
       "8            0.999934           0.857395            0.999934      4.680417   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.178278        0.006279     3.088342e-05  \n",
       "1        0.106040        0.007088     0.000000e+00  \n",
       "2        0.153332        0.005820     3.088342e-05  \n",
       "3        0.025862        0.002274     1.110223e-16  \n",
       "4        0.244252        0.004035     1.110223e-16  \n",
       "5        0.167357        0.001690     1.110223e-16  \n",
       "6        0.061454        0.002170     1.110223e-16  \n",
       "7        0.042717        0.002927     1.110223e-16  \n",
       "8        0.111873        0.004843     1.110223e-16  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results4 = pd.DataFrame(clf4.cv_results_)\n",
    "results4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that now the best performance is found at intermediate values suggesting good parameters to be found.\n",
    "We can still optimize the values in smaller range but this is not worth the effort since the change from the peak performance\n",
    "around the optimal paramater is too slow that suggests that any such optimization is not fruitful for the effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 : The Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the cross validation is perfomed we can load the model directly with `model = clf4.best_estimator_` \n",
    "Otherwise, we can train the model using the best parameters in the 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=140, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=140, n_jobs=8, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Noting the best parameter setting to be n_estimators = 140 and max_depth = 140. We can directly train \n",
    "# the best model for next time.\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=8, random_state=0 , n_estimators = 140 , max_depth = 140)\n",
    "\n",
    "vectors_train, vector_val,labels_train, labels_val = train_test_split(vectors_cv, labels_cv, test_size=0.11, random_state=42)\n",
    "model.fit(vectors_train,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 : Evaluation Criterion : Accuracy and Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 : Different Accuracies:\n",
    "\n",
    "It is interesting to note that even though the validation dataset is not use for training, the accuracy on val is certainly \n",
    "consistently a bit higher than the testing dataset which is an artefact of the fact that we are using it to indirectly train by \n",
    "choosing the best parameters through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 99.993\n",
      "Validation set Accuracy 85.423\n",
      "Test Accuracy 84.297\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy', np.round(model.score(vectors_train, labels_train)*100,3))\n",
    "print('Validation set Accuracy', np.round(model.score(vector_val,labels_val)*100,3))\n",
    "print('Test Accuracy', np.round(model.score(vector_test,labels_test)*100,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 : Confusion Matrix :\n",
    "\n",
    "We are using the library `pandas-ml` for directly plotting the confusion matrix. Same can also be achieved in scikit-learn although with much more lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x568190ef98>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAHZCAYAAAD5SiP6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5682e832b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_pred =model.predict(vector_test)\n",
    "cm = ConfusionMatrix(labels_test,labels_pred)\n",
    "cm.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model classify pretty well most of the categories.  \n",
    "The most category that the classifier missclassify is the 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.5 : Inspecting feature importances : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173762\n"
     ]
    }
   ],
   "source": [
    "print(len(model.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(data_all.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.00040000000000000002, 'alt.atheism'), (0.00020000000000000001, 'comp.graphics'), (0.0, 'talk.religion.misc'), (0.0, 'talk.politics.misc'), (0.0, 'talk.politics.mideast'), (0.0, 'talk.politics.guns'), (0.0, 'soc.religion.christian'), (0.0, 'sci.space'), (0.0, 'sci.med'), (0.0, 'sci.electronics'), (0.0, 'sci.crypt'), (0.0, 'rec.sport.hockey'), (0.0, 'rec.sport.baseball'), (0.0, 'rec.motorcycles'), (0.0, 'rec.autos'), (0.0, 'misc.forsale'), (0.0, 'comp.windows.x'), (0.0, 'comp.sys.mac.hardware'), (0.0, 'comp.sys.ibm.pc.hardware'), (0.0, 'comp.os.ms-windows.misc')]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(zip(map(lambda x: round(x, 4), model.feature_importances_), names), \n",
    "             reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
