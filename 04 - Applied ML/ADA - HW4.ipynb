{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Applied ML\n",
    "## Question 1: Propensity score matching\n",
    "### 0. Loading the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSW1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9930.0460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSW2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3595.8940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSW3</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24909.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NSW4</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7506.1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NSW5</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.7899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  treat  age  educ  black  hispan  married  nodegree  re74  re75  \\\n",
       "0  NSW1      1   37    11      1       0        1         1   0.0   0.0   \n",
       "1  NSW2      1   22     9      0       1        0         1   0.0   0.0   \n",
       "2  NSW3      1   30    12      1       0        0         0   0.0   0.0   \n",
       "3  NSW4      1   27    11      1       0        0         1   0.0   0.0   \n",
       "4  NSW5      1   33     8      1       0        0         1   0.0   0.0   \n",
       "\n",
       "         re78  \n",
       "0   9930.0460  \n",
       "1   3595.8940  \n",
       "2  24909.4500  \n",
       "3   7506.1460  \n",
       "4    289.7899  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lalonde_df = pd.read_csv('./lalonde.csv')\n",
    "lalonde_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A naive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x109a9d860>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMBwkSR2GgySp4/8DnYEt9zQGnFMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1099b9588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(lalonde_df.treat, lalonde_df.re78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: -0.0390327053015\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>non_treated</th>\n",
       "      <th>treated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6984.169742</td>\n",
       "      <td>6349.143530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7294.161791</td>\n",
       "      <td>7867.402218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>220.181300</td>\n",
       "      <td>485.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4975.505000</td>\n",
       "      <td>4232.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11688.820000</td>\n",
       "      <td>9642.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25564.670000</td>\n",
       "      <td>60307.930000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        non_treated       treated\n",
       "count    429.000000    185.000000\n",
       "mean    6984.169742   6349.143530\n",
       "std     7294.161791   7867.402218\n",
       "min        0.000000      0.000000\n",
       "25%      220.181300    485.229800\n",
       "50%     4975.505000   4232.309000\n",
       "75%    11688.820000   9642.999000\n",
       "max    25564.670000  60307.930000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treated_df = lalonde_df[lalonde_df.treat == 1]\n",
    "control_df = lalonde_df[lalonde_df.treat == 0]\n",
    "\n",
    "print(\"Correlation:\", lalonde_df.treat.corr(lalonde_df.re78))\n",
    "\n",
    "pd.DataFrame(dict(treated = treated_df.re78.describe(), \n",
    "                  non_treated=control_df.re78.describe()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive interpretation :  \n",
    "The treatment seems to have no effect to the outcome\n",
    "\n",
    "### 2. A closer look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>185.0</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.816216</td>\n",
       "      <td>10.345946</td>\n",
       "      <td>0.843243</td>\n",
       "      <td>0.059459</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.708108</td>\n",
       "      <td>2095.573689</td>\n",
       "      <td>1532.055314</td>\n",
       "      <td>6349.143530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.155019</td>\n",
       "      <td>2.010650</td>\n",
       "      <td>0.364558</td>\n",
       "      <td>0.237124</td>\n",
       "      <td>0.392722</td>\n",
       "      <td>0.455867</td>\n",
       "      <td>4886.620353</td>\n",
       "      <td>3219.250870</td>\n",
       "      <td>7867.402218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>485.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4232.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1291.468000</td>\n",
       "      <td>1817.284000</td>\n",
       "      <td>9642.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35040.070000</td>\n",
       "      <td>25142.240000</td>\n",
       "      <td>60307.930000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       treat         age        educ       black      hispan     married  \\\n",
       "count  185.0  185.000000  185.000000  185.000000  185.000000  185.000000   \n",
       "mean     1.0   25.816216   10.345946    0.843243    0.059459    0.189189   \n",
       "std      0.0    7.155019    2.010650    0.364558    0.237124    0.392722   \n",
       "min      1.0   17.000000    4.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.0   20.000000    9.000000    1.000000    0.000000    0.000000   \n",
       "50%      1.0   25.000000   11.000000    1.000000    0.000000    0.000000   \n",
       "75%      1.0   29.000000   12.000000    1.000000    0.000000    0.000000   \n",
       "max      1.0   48.000000   16.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree          re74          re75          re78  \n",
       "count  185.000000    185.000000    185.000000    185.000000  \n",
       "mean     0.708108   2095.573689   1532.055314   6349.143530  \n",
       "std      0.455867   4886.620353   3219.250870   7867.402218  \n",
       "min      0.000000      0.000000      0.000000      0.000000  \n",
       "25%      0.000000      0.000000      0.000000    485.229800  \n",
       "50%      1.000000      0.000000      0.000000   4232.309000  \n",
       "75%      1.000000   1291.468000   1817.284000   9642.999000  \n",
       "max      1.000000  35040.070000  25142.240000  60307.930000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treated_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.0</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.030303</td>\n",
       "      <td>10.235431</td>\n",
       "      <td>0.202797</td>\n",
       "      <td>0.142191</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.596737</td>\n",
       "      <td>5619.236506</td>\n",
       "      <td>2466.484443</td>\n",
       "      <td>6984.169742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.786653</td>\n",
       "      <td>2.855238</td>\n",
       "      <td>0.402552</td>\n",
       "      <td>0.349654</td>\n",
       "      <td>0.500419</td>\n",
       "      <td>0.491126</td>\n",
       "      <td>6788.750796</td>\n",
       "      <td>3291.996183</td>\n",
       "      <td>7294.161791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2547.047000</td>\n",
       "      <td>1086.726000</td>\n",
       "      <td>4975.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9277.128000</td>\n",
       "      <td>3881.419000</td>\n",
       "      <td>11688.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25862.320000</td>\n",
       "      <td>18347.230000</td>\n",
       "      <td>25564.670000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       treat         age        educ       black      hispan     married  \\\n",
       "count  429.0  429.000000  429.000000  429.000000  429.000000  429.000000   \n",
       "mean     0.0   28.030303   10.235431    0.202797    0.142191    0.512821   \n",
       "std      0.0   10.786653    2.855238    0.402552    0.349654    0.500419   \n",
       "min      0.0   16.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.0   19.000000    9.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.0   25.000000   11.000000    0.000000    0.000000    1.000000   \n",
       "75%      0.0   35.000000   12.000000    0.000000    0.000000    1.000000   \n",
       "max      0.0   55.000000   18.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree          re74          re75          re78  \n",
       "count  429.000000    429.000000    429.000000    429.000000  \n",
       "mean     0.596737   5619.236506   2466.484443   6984.169742  \n",
       "std      0.491126   6788.750796   3291.996183   7294.161791  \n",
       "min      0.000000      0.000000      0.000000      0.000000  \n",
       "25%      0.000000      0.000000      0.000000    220.181300  \n",
       "50%      1.000000   2547.047000   1086.726000   4975.505000  \n",
       "75%      1.000000   9277.128000   3881.419000  11688.820000  \n",
       "max      1.000000  25862.320000  18347.230000  25564.670000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treated group contains more \"black\" people (84%) than the the control group (only 20%).  \n",
    "Moreover, the treated people were earn less money before the treatment 1974-1975 than the control people.\n",
    "Furthermore, the ratio of married people in the treated group is less than the control group (19% against 51%).\n",
    "Despite of having more or less the same years of education, the treated group has more no degree than the control one.  \n",
    "\n",
    "**It seems that the two groups dont come from the same distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1078fecc0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhAmSXyeF3LAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109a50588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAELCAYAAAA1AlaNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIa+Ukq5iIa+Ukq5yP8P1dNen4wJPUQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107889b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\n",
    "sns.distplot(treated_df.age, ax=ax1)\n",
    "sns.distplot(control_df.age, ax=ax2)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\n",
    "sns.distplot(treated_df.educ, ax=ax1)\n",
    "sns.distplot(control_df.educ, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the two groups dont come frome the same distribution, the naive interpretation is not acceptable.\n",
    "\n",
    "### 3. A propensity score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "logistic= linear_model.LogisticRegression()\n",
    "\n",
    "y = lalonde_df.treat\n",
    "x = lalonde_df.drop(['treat', 'id', 're78'], axis=1)\n",
    "\n",
    "logistic.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "      <th>propensity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSW1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9930.0460</td>\n",
       "      <td>0.443350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSW2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3595.8940</td>\n",
       "      <td>0.144660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSW3</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24909.4500</td>\n",
       "      <td>0.722355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NSW4</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7506.1460</td>\n",
       "      <td>0.664151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NSW5</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.7899</td>\n",
       "      <td>0.698286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  treat  age  educ  black  hispan  married  nodegree  re74  re75  \\\n",
       "0  NSW1      1   37    11      1       0        1         1   0.0   0.0   \n",
       "1  NSW2      1   22     9      0       1        0         1   0.0   0.0   \n",
       "2  NSW3      1   30    12      1       0        0         0   0.0   0.0   \n",
       "3  NSW4      1   27    11      1       0        0         1   0.0   0.0   \n",
       "4  NSW5      1   33     8      1       0        0         1   0.0   0.0   \n",
       "\n",
       "         re78  propensity_score  \n",
       "0   9930.0460          0.443350  \n",
       "1   3595.8940          0.144660  \n",
       "2  24909.4500          0.722355  \n",
       "3   7506.1460          0.664151  \n",
       "4    289.7899          0.698286  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba = logistic.predict_proba(x)\n",
    "propensity_score = proba[:, 1]\n",
    "\n",
    "propensity_score_series = pd.Series(propensity_score)\n",
    "propensity_df = lalonde_df.assign(propensity_score=propensity_score_series.values)\n",
    "propensity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 : Applied Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and loading the data into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_all = fetch_20newsgroups(subset='all')\n",
    "labels_all = data_all.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the text using TF-IDF features and default Scikit-learn functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has  18846  examples with each examples having  173762  dimensions.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors_all = vectorizer.fit_transform(data_all.data)\n",
    "print('Dataset has ', vectors_all.shape[0] , ' examples with each examples having ', vectors_all.shape[1] , ' dimensions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Cross Validation bench for finding the optimal parameter\n",
    "\n",
    "This whole section needs to be run just once for finding the optimal params for andom Forest Classfier. Once found, we can directly jump to the next - Final model section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data splits\n",
    "\n",
    "The training data set is 10% of the complete data where the total cross validation data is 90%. We have used scikit learn functionality for this but the same can also be achieved with few more lines of code by joining the labels with data, slicing using pandas/numpy and separating the labels afterwards.\n",
    "\n",
    "As for cross-validation , we train each model thrice on a randomly chosen subset of the cross-val data. Notice that we have chosen the test_size as 0.11 in Shuffle split ( although called test_size this is the size of the cross_val dataset) because it ~115 of the 90% of the total data. Thus totalling to 9.9%~10% of the whole data. Otherwise, we would've got 9% of the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors_cv, vector_test,labels_cv, labels_test = train_test_split( vectors_all, labels_all, test_size=0.1, random_state=42)\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.11, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Starting Cross-Validation\n",
    "\n",
    "Starting with a wide inital guess of params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [20 ,30, 40, 50, 60, 70],\n",
    "              'n_estimators': [5, 10, 15, 20, 30, 40, 50], }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `%time` magical commad allows to track the actual time taken to complete. `n_jobs` is an additional parameter \n",
    "to specify the number of cores to use to compute. Can be adjused as per user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=0, test_size=0.1, train_size=None),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=8, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15, 20, 30, 40, 50], 'max_depth': [20, 30, 40, 50, 60, 70]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf =  GridSearchCV(RandomForestClassifier(random_state=0 , n_jobs = 8), param_grid , cv = cv)\n",
    "%time clf.fit(vectors_cv, labels_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining a clean table for performance measures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.485441</td>\n",
       "      <td>0.165653</td>\n",
       "      <td>0.411707</td>\n",
       "      <td>0.487574</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_estimators': 5, 'max_depth': 20}</td>\n",
       "      <td>42</td>\n",
       "      <td>0.424278</td>\n",
       "      <td>0.494824</td>\n",
       "      <td>0.426635</td>\n",
       "      <td>0.495545</td>\n",
       "      <td>0.384207</td>\n",
       "      <td>0.472353</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.006569</td>\n",
       "      <td>0.019469</td>\n",
       "      <td>0.010767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.117120</td>\n",
       "      <td>0.156975</td>\n",
       "      <td>0.509134</td>\n",
       "      <td>0.625590</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 20}</td>\n",
       "      <td>39</td>\n",
       "      <td>0.503241</td>\n",
       "      <td>0.623755</td>\n",
       "      <td>0.532115</td>\n",
       "      <td>0.634762</td>\n",
       "      <td>0.492045</td>\n",
       "      <td>0.618252</td>\n",
       "      <td>0.125276</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.016881</td>\n",
       "      <td>0.006864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.245847</td>\n",
       "      <td>0.151944</td>\n",
       "      <td>0.574150</td>\n",
       "      <td>0.701083</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_estimators': 15, 'max_depth': 20}</td>\n",
       "      <td>35</td>\n",
       "      <td>0.575722</td>\n",
       "      <td>0.700865</td>\n",
       "      <td>0.591632</td>\n",
       "      <td>0.705516</td>\n",
       "      <td>0.555097</td>\n",
       "      <td>0.696868</td>\n",
       "      <td>0.084782</td>\n",
       "      <td>0.008742</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.003534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.826435</td>\n",
       "      <td>0.149638</td>\n",
       "      <td>0.624435</td>\n",
       "      <td>0.751114</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 20}</td>\n",
       "      <td>32</td>\n",
       "      <td>0.629935</td>\n",
       "      <td>0.754782</td>\n",
       "      <td>0.635828</td>\n",
       "      <td>0.751310</td>\n",
       "      <td>0.607543</td>\n",
       "      <td>0.747248</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>0.012219</td>\n",
       "      <td>0.012185</td>\n",
       "      <td>0.003079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.272358</td>\n",
       "      <td>0.140097</td>\n",
       "      <td>0.675309</td>\n",
       "      <td>0.806560</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 20}</td>\n",
       "      <td>26</td>\n",
       "      <td>0.674131</td>\n",
       "      <td>0.797956</td>\n",
       "      <td>0.676488</td>\n",
       "      <td>0.810273</td>\n",
       "      <td>0.675309</td>\n",
       "      <td>0.811452</td>\n",
       "      <td>0.071779</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.006103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.004456</td>\n",
       "      <td>0.204342</td>\n",
       "      <td>0.709094</td>\n",
       "      <td>0.833923</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 20}</td>\n",
       "      <td>20</td>\n",
       "      <td>0.705952</td>\n",
       "      <td>0.829468</td>\n",
       "      <td>0.715380</td>\n",
       "      <td>0.839557</td>\n",
       "      <td>0.705952</td>\n",
       "      <td>0.832744</td>\n",
       "      <td>0.294505</td>\n",
       "      <td>0.049824</td>\n",
       "      <td>0.004445</td>\n",
       "      <td>0.004202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.567097</td>\n",
       "      <td>0.241132</td>\n",
       "      <td>0.725594</td>\n",
       "      <td>0.850585</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 20}</td>\n",
       "      <td>19</td>\n",
       "      <td>0.723630</td>\n",
       "      <td>0.844864</td>\n",
       "      <td>0.723041</td>\n",
       "      <td>0.854232</td>\n",
       "      <td>0.730112</td>\n",
       "      <td>0.852660</td>\n",
       "      <td>0.138987</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>0.004096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.885443</td>\n",
       "      <td>0.161891</td>\n",
       "      <td>0.465527</td>\n",
       "      <td>0.603249</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_estimators': 5, 'max_depth': 30}</td>\n",
       "      <td>41</td>\n",
       "      <td>0.469063</td>\n",
       "      <td>0.609342</td>\n",
       "      <td>0.467885</td>\n",
       "      <td>0.599515</td>\n",
       "      <td>0.459635</td>\n",
       "      <td>0.600891</td>\n",
       "      <td>0.044893</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.004345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.853358</td>\n",
       "      <td>0.161247</td>\n",
       "      <td>0.576704</td>\n",
       "      <td>0.747773</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 30}</td>\n",
       "      <td>34</td>\n",
       "      <td>0.577490</td>\n",
       "      <td>0.746004</td>\n",
       "      <td>0.578079</td>\n",
       "      <td>0.749017</td>\n",
       "      <td>0.574543</td>\n",
       "      <td>0.748297</td>\n",
       "      <td>0.065387</td>\n",
       "      <td>0.009129</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.993183</td>\n",
       "      <td>0.137166</td>\n",
       "      <td>0.641917</td>\n",
       "      <td>0.814968</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_estimators': 15, 'max_depth': 30}</td>\n",
       "      <td>30</td>\n",
       "      <td>0.626400</td>\n",
       "      <td>0.811386</td>\n",
       "      <td>0.654685</td>\n",
       "      <td>0.812959</td>\n",
       "      <td>0.644667</td>\n",
       "      <td>0.820558</td>\n",
       "      <td>0.172783</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>0.004005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.801513</td>\n",
       "      <td>0.139816</td>\n",
       "      <td>0.679041</td>\n",
       "      <td>0.852179</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 30}</td>\n",
       "      <td>25</td>\n",
       "      <td>0.677077</td>\n",
       "      <td>0.849515</td>\n",
       "      <td>0.674131</td>\n",
       "      <td>0.850956</td>\n",
       "      <td>0.685916</td>\n",
       "      <td>0.856067</td>\n",
       "      <td>0.127854</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>0.002811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.583630</td>\n",
       "      <td>0.216298</td>\n",
       "      <td>0.731683</td>\n",
       "      <td>0.893453</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 30}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.727166</td>\n",
       "      <td>0.890592</td>\n",
       "      <td>0.719505</td>\n",
       "      <td>0.892361</td>\n",
       "      <td>0.748379</td>\n",
       "      <td>0.897406</td>\n",
       "      <td>0.282430</td>\n",
       "      <td>0.062495</td>\n",
       "      <td>0.012213</td>\n",
       "      <td>0.002887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.139858</td>\n",
       "      <td>0.216382</td>\n",
       "      <td>0.750147</td>\n",
       "      <td>0.911513</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 30}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.743665</td>\n",
       "      <td>0.910443</td>\n",
       "      <td>0.746022</td>\n",
       "      <td>0.910443</td>\n",
       "      <td>0.760754</td>\n",
       "      <td>0.913653</td>\n",
       "      <td>0.253957</td>\n",
       "      <td>0.063661</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>0.001513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.571270</td>\n",
       "      <td>0.195859</td>\n",
       "      <td>0.765272</td>\n",
       "      <td>0.922454</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 30}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.761933</td>\n",
       "      <td>0.923611</td>\n",
       "      <td>0.748379</td>\n",
       "      <td>0.918436</td>\n",
       "      <td>0.785504</td>\n",
       "      <td>0.925314</td>\n",
       "      <td>0.264881</td>\n",
       "      <td>0.051579</td>\n",
       "      <td>0.015339</td>\n",
       "      <td>0.002925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.920082</td>\n",
       "      <td>0.147718</td>\n",
       "      <td>0.487331</td>\n",
       "      <td>0.692807</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_estimators': 5, 'max_depth': 40}</td>\n",
       "      <td>40</td>\n",
       "      <td>0.507955</td>\n",
       "      <td>0.706237</td>\n",
       "      <td>0.480259</td>\n",
       "      <td>0.687828</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.684355</td>\n",
       "      <td>0.795735</td>\n",
       "      <td>0.020983</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>0.009602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.422347</td>\n",
       "      <td>0.142671</td>\n",
       "      <td>0.605578</td>\n",
       "      <td>0.823157</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 40}</td>\n",
       "      <td>33</td>\n",
       "      <td>0.610489</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.604596</td>\n",
       "      <td>0.820362</td>\n",
       "      <td>0.601650</td>\n",
       "      <td>0.818920</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.005007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.739362</td>\n",
       "      <td>0.133749</td>\n",
       "      <td>0.666470</td>\n",
       "      <td>0.878887</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_estimators': 15, 'max_depth': 40}</td>\n",
       "      <td>27</td>\n",
       "      <td>0.663524</td>\n",
       "      <td>0.878538</td>\n",
       "      <td>0.667649</td>\n",
       "      <td>0.880831</td>\n",
       "      <td>0.668238</td>\n",
       "      <td>0.877293</td>\n",
       "      <td>0.201872</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.001465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.070293</td>\n",
       "      <td>0.126705</td>\n",
       "      <td>0.699273</td>\n",
       "      <td>0.910159</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 40}</td>\n",
       "      <td>23</td>\n",
       "      <td>0.693577</td>\n",
       "      <td>0.910705</td>\n",
       "      <td>0.693577</td>\n",
       "      <td>0.912343</td>\n",
       "      <td>0.710666</td>\n",
       "      <td>0.907429</td>\n",
       "      <td>1.048325</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.008056</td>\n",
       "      <td>0.002043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.703439</td>\n",
       "      <td>0.124589</td>\n",
       "      <td>0.743272</td>\n",
       "      <td>0.936648</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 40}</td>\n",
       "      <td>14</td>\n",
       "      <td>0.747201</td>\n",
       "      <td>0.933766</td>\n",
       "      <td>0.736005</td>\n",
       "      <td>0.939531</td>\n",
       "      <td>0.746612</td>\n",
       "      <td>0.936648</td>\n",
       "      <td>0.273186</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.002354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.646753</td>\n",
       "      <td>0.215155</td>\n",
       "      <td>0.769790</td>\n",
       "      <td>0.949358</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 40}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.773718</td>\n",
       "      <td>0.948244</td>\n",
       "      <td>0.758986</td>\n",
       "      <td>0.950668</td>\n",
       "      <td>0.776665</td>\n",
       "      <td>0.949161</td>\n",
       "      <td>0.534332</td>\n",
       "      <td>0.069268</td>\n",
       "      <td>0.007733</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.960682</td>\n",
       "      <td>0.273598</td>\n",
       "      <td>0.786290</td>\n",
       "      <td>0.957656</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.796111</td>\n",
       "      <td>0.957613</td>\n",
       "      <td>0.774308</td>\n",
       "      <td>0.957023</td>\n",
       "      <td>0.788450</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.170681</td>\n",
       "      <td>0.060580</td>\n",
       "      <td>0.009031</td>\n",
       "      <td>0.000536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.748060</td>\n",
       "      <td>0.148117</td>\n",
       "      <td>0.517384</td>\n",
       "      <td>0.759674</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_estimators': 5, 'max_depth': 50}</td>\n",
       "      <td>38</td>\n",
       "      <td>0.535062</td>\n",
       "      <td>0.772209</td>\n",
       "      <td>0.525633</td>\n",
       "      <td>0.755962</td>\n",
       "      <td>0.491456</td>\n",
       "      <td>0.750852</td>\n",
       "      <td>0.674180</td>\n",
       "      <td>0.010698</td>\n",
       "      <td>0.018734</td>\n",
       "      <td>0.009106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.420199</td>\n",
       "      <td>0.168990</td>\n",
       "      <td>0.630328</td>\n",
       "      <td>0.877511</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 50}</td>\n",
       "      <td>31</td>\n",
       "      <td>0.635239</td>\n",
       "      <td>0.880372</td>\n",
       "      <td>0.639364</td>\n",
       "      <td>0.874410</td>\n",
       "      <td>0.616382</td>\n",
       "      <td>0.877752</td>\n",
       "      <td>0.190985</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>0.002440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.748087</td>\n",
       "      <td>0.130341</td>\n",
       "      <td>0.688077</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_estimators': 15, 'max_depth': 50}</td>\n",
       "      <td>24</td>\n",
       "      <td>0.684148</td>\n",
       "      <td>0.923415</td>\n",
       "      <td>0.698291</td>\n",
       "      <td>0.923808</td>\n",
       "      <td>0.681791</td>\n",
       "      <td>0.926363</td>\n",
       "      <td>0.033240</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.007286</td>\n",
       "      <td>0.001307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.511441</td>\n",
       "      <td>0.131921</td>\n",
       "      <td>0.726183</td>\n",
       "      <td>0.946344</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 50}</td>\n",
       "      <td>18</td>\n",
       "      <td>0.720094</td>\n",
       "      <td>0.946803</td>\n",
       "      <td>0.724808</td>\n",
       "      <td>0.945689</td>\n",
       "      <td>0.733648</td>\n",
       "      <td>0.946541</td>\n",
       "      <td>0.065602</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.000475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.752810</td>\n",
       "      <td>0.123906</td>\n",
       "      <td>0.761540</td>\n",
       "      <td>0.964776</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 50}</td>\n",
       "      <td>12</td>\n",
       "      <td>0.758397</td>\n",
       "      <td>0.965671</td>\n",
       "      <td>0.753683</td>\n",
       "      <td>0.963968</td>\n",
       "      <td>0.772540</td>\n",
       "      <td>0.964688</td>\n",
       "      <td>0.161436</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.000698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.149021</td>\n",
       "      <td>0.213565</td>\n",
       "      <td>0.776665</td>\n",
       "      <td>0.974035</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 50}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.765468</td>\n",
       "      <td>0.974384</td>\n",
       "      <td>0.776665</td>\n",
       "      <td>0.972419</td>\n",
       "      <td>0.787861</td>\n",
       "      <td>0.975301</td>\n",
       "      <td>0.077515</td>\n",
       "      <td>0.064190</td>\n",
       "      <td>0.009142</td>\n",
       "      <td>0.001202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.674253</td>\n",
       "      <td>0.194212</td>\n",
       "      <td>0.792772</td>\n",
       "      <td>0.978315</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 50}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.789629</td>\n",
       "      <td>0.978774</td>\n",
       "      <td>0.787861</td>\n",
       "      <td>0.977136</td>\n",
       "      <td>0.800825</td>\n",
       "      <td>0.979036</td>\n",
       "      <td>0.998968</td>\n",
       "      <td>0.047472</td>\n",
       "      <td>0.005740</td>\n",
       "      <td>0.000841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.130178</td>\n",
       "      <td>0.155223</td>\n",
       "      <td>0.537419</td>\n",
       "      <td>0.810731</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_estimators': 5, 'max_depth': 60}</td>\n",
       "      <td>36</td>\n",
       "      <td>0.549794</td>\n",
       "      <td>0.830123</td>\n",
       "      <td>0.533883</td>\n",
       "      <td>0.798546</td>\n",
       "      <td>0.528580</td>\n",
       "      <td>0.803525</td>\n",
       "      <td>0.136971</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.013862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.734597</td>\n",
       "      <td>0.171287</td>\n",
       "      <td>0.658417</td>\n",
       "      <td>0.912277</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 60}</td>\n",
       "      <td>28</td>\n",
       "      <td>0.653506</td>\n",
       "      <td>0.917191</td>\n",
       "      <td>0.667060</td>\n",
       "      <td>0.904481</td>\n",
       "      <td>0.654685</td>\n",
       "      <td>0.915160</td>\n",
       "      <td>0.041833</td>\n",
       "      <td>0.005858</td>\n",
       "      <td>0.006130</td>\n",
       "      <td>0.005575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.226751</td>\n",
       "      <td>0.133294</td>\n",
       "      <td>0.705559</td>\n",
       "      <td>0.948703</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_estimators': 15, 'max_depth': 60}</td>\n",
       "      <td>22</td>\n",
       "      <td>0.695345</td>\n",
       "      <td>0.952503</td>\n",
       "      <td>0.723041</td>\n",
       "      <td>0.943265</td>\n",
       "      <td>0.698291</td>\n",
       "      <td>0.950341</td>\n",
       "      <td>0.075408</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.012420</td>\n",
       "      <td>0.003945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.393644</td>\n",
       "      <td>0.125001</td>\n",
       "      <td>0.737183</td>\n",
       "      <td>0.967178</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 60}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.728933</td>\n",
       "      <td>0.970846</td>\n",
       "      <td>0.741308</td>\n",
       "      <td>0.964819</td>\n",
       "      <td>0.741308</td>\n",
       "      <td>0.965867</td>\n",
       "      <td>0.048445</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>0.002629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7.703230</td>\n",
       "      <td>0.173632</td>\n",
       "      <td>0.773915</td>\n",
       "      <td>0.978883</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 60}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.780790</td>\n",
       "      <td>0.980870</td>\n",
       "      <td>0.768415</td>\n",
       "      <td>0.977725</td>\n",
       "      <td>0.772540</td>\n",
       "      <td>0.978053</td>\n",
       "      <td>0.156272</td>\n",
       "      <td>0.072542</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.001412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9.351722</td>\n",
       "      <td>0.124458</td>\n",
       "      <td>0.792575</td>\n",
       "      <td>0.984954</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 60}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.802004</td>\n",
       "      <td>0.986832</td>\n",
       "      <td>0.786093</td>\n",
       "      <td>0.982835</td>\n",
       "      <td>0.789629</td>\n",
       "      <td>0.985194</td>\n",
       "      <td>0.127502</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.006821</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10.496654</td>\n",
       "      <td>0.257629</td>\n",
       "      <td>0.804164</td>\n",
       "      <td>0.987552</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 60}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.806718</td>\n",
       "      <td>0.988994</td>\n",
       "      <td>0.799057</td>\n",
       "      <td>0.985718</td>\n",
       "      <td>0.806718</td>\n",
       "      <td>0.987945</td>\n",
       "      <td>1.254174</td>\n",
       "      <td>0.018957</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.001366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3.783091</td>\n",
       "      <td>0.149218</td>\n",
       "      <td>0.532115</td>\n",
       "      <td>0.850935</td>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_estimators': 5, 'max_depth': 70}</td>\n",
       "      <td>37</td>\n",
       "      <td>0.532705</td>\n",
       "      <td>0.861701</td>\n",
       "      <td>0.532115</td>\n",
       "      <td>0.841916</td>\n",
       "      <td>0.531526</td>\n",
       "      <td>0.849188</td>\n",
       "      <td>0.172407</td>\n",
       "      <td>0.011259</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.008171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.747668</td>\n",
       "      <td>0.154155</td>\n",
       "      <td>0.646042</td>\n",
       "      <td>0.941060</td>\n",
       "      <td>70</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 70}</td>\n",
       "      <td>29</td>\n",
       "      <td>0.637006</td>\n",
       "      <td>0.944182</td>\n",
       "      <td>0.653506</td>\n",
       "      <td>0.935469</td>\n",
       "      <td>0.647613</td>\n",
       "      <td>0.943527</td>\n",
       "      <td>0.659774</td>\n",
       "      <td>0.020343</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>0.003962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.749373</td>\n",
       "      <td>0.138602</td>\n",
       "      <td>0.705952</td>\n",
       "      <td>0.970017</td>\n",
       "      <td>70</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_estimators': 15, 'max_depth': 70}</td>\n",
       "      <td>21</td>\n",
       "      <td>0.692398</td>\n",
       "      <td>0.972681</td>\n",
       "      <td>0.715969</td>\n",
       "      <td>0.967309</td>\n",
       "      <td>0.709487</td>\n",
       "      <td>0.970060</td>\n",
       "      <td>0.126266</td>\n",
       "      <td>0.023520</td>\n",
       "      <td>0.009942</td>\n",
       "      <td>0.002193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6.762719</td>\n",
       "      <td>0.140030</td>\n",
       "      <td>0.738165</td>\n",
       "      <td>0.981350</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 70}</td>\n",
       "      <td>15</td>\n",
       "      <td>0.725987</td>\n",
       "      <td>0.983491</td>\n",
       "      <td>0.743076</td>\n",
       "      <td>0.979756</td>\n",
       "      <td>0.745433</td>\n",
       "      <td>0.980805</td>\n",
       "      <td>0.792023</td>\n",
       "      <td>0.023083</td>\n",
       "      <td>0.008665</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>8.987674</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.778629</td>\n",
       "      <td>0.988994</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 70}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.775486</td>\n",
       "      <td>0.990959</td>\n",
       "      <td>0.772540</td>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.787861</td>\n",
       "      <td>0.986701</td>\n",
       "      <td>0.132610</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.006638</td>\n",
       "      <td>0.001754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10.700832</td>\n",
       "      <td>0.258806</td>\n",
       "      <td>0.797682</td>\n",
       "      <td>0.992444</td>\n",
       "      <td>70</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 70}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.789629</td>\n",
       "      <td>0.993449</td>\n",
       "      <td>0.797879</td>\n",
       "      <td>0.992859</td>\n",
       "      <td>0.805539</td>\n",
       "      <td>0.991025</td>\n",
       "      <td>0.155441</td>\n",
       "      <td>0.019988</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>0.001032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>13.170077</td>\n",
       "      <td>0.258905</td>\n",
       "      <td>0.807307</td>\n",
       "      <td>0.993470</td>\n",
       "      <td>70</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 70}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800236</td>\n",
       "      <td>0.994693</td>\n",
       "      <td>0.806718</td>\n",
       "      <td>0.993776</td>\n",
       "      <td>0.814968</td>\n",
       "      <td>0.991942</td>\n",
       "      <td>0.082882</td>\n",
       "      <td>0.021130</td>\n",
       "      <td>0.006029</td>\n",
       "      <td>0.001144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0        1.485441         0.165653         0.411707          0.487574   \n",
       "1        2.117120         0.156975         0.509134          0.625590   \n",
       "2        2.245847         0.151944         0.574150          0.701083   \n",
       "3        2.826435         0.149638         0.624435          0.751114   \n",
       "4        3.272358         0.140097         0.675309          0.806560   \n",
       "5        4.004456         0.204342         0.709094          0.833923   \n",
       "6        4.567097         0.241132         0.725594          0.850585   \n",
       "7        1.885443         0.161891         0.465527          0.603249   \n",
       "8        2.853358         0.161247         0.576704          0.747773   \n",
       "9        2.993183         0.137166         0.641917          0.814968   \n",
       "10       3.801513         0.139816         0.679041          0.852179   \n",
       "11       4.583630         0.216298         0.731683          0.893453   \n",
       "12       5.139858         0.216382         0.750147          0.911513   \n",
       "13       5.571270         0.195859         0.765272          0.922454   \n",
       "14       1.920082         0.147718         0.487331          0.692807   \n",
       "15       3.422347         0.142671         0.605578          0.823157   \n",
       "16       4.739362         0.133749         0.666470          0.878887   \n",
       "17       4.070293         0.126705         0.699273          0.910159   \n",
       "18       5.703439         0.124589         0.743272          0.936648   \n",
       "19       7.646753         0.215155         0.769790          0.949358   \n",
       "20       8.960682         0.273598         0.786290          0.957656   \n",
       "21       2.748060         0.148117         0.517384          0.759674   \n",
       "22       4.420199         0.168990         0.630328          0.877511   \n",
       "23       4.748087         0.130341         0.688077          0.924528   \n",
       "24       5.511441         0.131921         0.726183          0.946344   \n",
       "25       6.752810         0.123906         0.761540          0.964776   \n",
       "26       8.149021         0.213565         0.776665          0.974035   \n",
       "27       8.674253         0.194212         0.792772          0.978315   \n",
       "28       3.130178         0.155223         0.537419          0.810731   \n",
       "29       4.734597         0.171287         0.658417          0.912277   \n",
       "30       5.226751         0.133294         0.705559          0.948703   \n",
       "31       6.393644         0.125001         0.737183          0.967178   \n",
       "32       7.703230         0.173632         0.773915          0.978883   \n",
       "33       9.351722         0.124458         0.792575          0.984954   \n",
       "34      10.496654         0.257629         0.804164          0.987552   \n",
       "35       3.783091         0.149218         0.532115          0.850935   \n",
       "36       5.747668         0.154155         0.646042          0.941060   \n",
       "37       5.749373         0.138602         0.705952          0.970017   \n",
       "38       6.762719         0.140030         0.738165          0.981350   \n",
       "39       8.987674         0.125300         0.778629          0.988994   \n",
       "40      10.700832         0.258806         0.797682          0.992444   \n",
       "41      13.170077         0.258905         0.807307          0.993470   \n",
       "\n",
       "   param_max_depth param_n_estimators                                 params  \\\n",
       "0               20                  5   {'n_estimators': 5, 'max_depth': 20}   \n",
       "1               20                 10  {'n_estimators': 10, 'max_depth': 20}   \n",
       "2               20                 15  {'n_estimators': 15, 'max_depth': 20}   \n",
       "3               20                 20  {'n_estimators': 20, 'max_depth': 20}   \n",
       "4               20                 30  {'n_estimators': 30, 'max_depth': 20}   \n",
       "5               20                 40  {'n_estimators': 40, 'max_depth': 20}   \n",
       "6               20                 50  {'n_estimators': 50, 'max_depth': 20}   \n",
       "7               30                  5   {'n_estimators': 5, 'max_depth': 30}   \n",
       "8               30                 10  {'n_estimators': 10, 'max_depth': 30}   \n",
       "9               30                 15  {'n_estimators': 15, 'max_depth': 30}   \n",
       "10              30                 20  {'n_estimators': 20, 'max_depth': 30}   \n",
       "11              30                 30  {'n_estimators': 30, 'max_depth': 30}   \n",
       "12              30                 40  {'n_estimators': 40, 'max_depth': 30}   \n",
       "13              30                 50  {'n_estimators': 50, 'max_depth': 30}   \n",
       "14              40                  5   {'n_estimators': 5, 'max_depth': 40}   \n",
       "15              40                 10  {'n_estimators': 10, 'max_depth': 40}   \n",
       "16              40                 15  {'n_estimators': 15, 'max_depth': 40}   \n",
       "17              40                 20  {'n_estimators': 20, 'max_depth': 40}   \n",
       "18              40                 30  {'n_estimators': 30, 'max_depth': 40}   \n",
       "19              40                 40  {'n_estimators': 40, 'max_depth': 40}   \n",
       "20              40                 50  {'n_estimators': 50, 'max_depth': 40}   \n",
       "21              50                  5   {'n_estimators': 5, 'max_depth': 50}   \n",
       "22              50                 10  {'n_estimators': 10, 'max_depth': 50}   \n",
       "23              50                 15  {'n_estimators': 15, 'max_depth': 50}   \n",
       "24              50                 20  {'n_estimators': 20, 'max_depth': 50}   \n",
       "25              50                 30  {'n_estimators': 30, 'max_depth': 50}   \n",
       "26              50                 40  {'n_estimators': 40, 'max_depth': 50}   \n",
       "27              50                 50  {'n_estimators': 50, 'max_depth': 50}   \n",
       "28              60                  5   {'n_estimators': 5, 'max_depth': 60}   \n",
       "29              60                 10  {'n_estimators': 10, 'max_depth': 60}   \n",
       "30              60                 15  {'n_estimators': 15, 'max_depth': 60}   \n",
       "31              60                 20  {'n_estimators': 20, 'max_depth': 60}   \n",
       "32              60                 30  {'n_estimators': 30, 'max_depth': 60}   \n",
       "33              60                 40  {'n_estimators': 40, 'max_depth': 60}   \n",
       "34              60                 50  {'n_estimators': 50, 'max_depth': 60}   \n",
       "35              70                  5   {'n_estimators': 5, 'max_depth': 70}   \n",
       "36              70                 10  {'n_estimators': 10, 'max_depth': 70}   \n",
       "37              70                 15  {'n_estimators': 15, 'max_depth': 70}   \n",
       "38              70                 20  {'n_estimators': 20, 'max_depth': 70}   \n",
       "39              70                 30  {'n_estimators': 30, 'max_depth': 70}   \n",
       "40              70                 40  {'n_estimators': 40, 'max_depth': 70}   \n",
       "41              70                 50  {'n_estimators': 50, 'max_depth': 70}   \n",
       "\n",
       "    rank_test_score  split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0                42           0.424278            0.494824           0.426635   \n",
       "1                39           0.503241            0.623755           0.532115   \n",
       "2                35           0.575722            0.700865           0.591632   \n",
       "3                32           0.629935            0.754782           0.635828   \n",
       "4                26           0.674131            0.797956           0.676488   \n",
       "5                20           0.705952            0.829468           0.715380   \n",
       "6                19           0.723630            0.844864           0.723041   \n",
       "7                41           0.469063            0.609342           0.467885   \n",
       "8                34           0.577490            0.746004           0.578079   \n",
       "9                30           0.626400            0.811386           0.654685   \n",
       "10               25           0.677077            0.849515           0.674131   \n",
       "11               17           0.727166            0.890592           0.719505   \n",
       "12               13           0.743665            0.910443           0.746022   \n",
       "13               11           0.761933            0.923611           0.748379   \n",
       "14               40           0.507955            0.706237           0.480259   \n",
       "15               33           0.610489            0.830189           0.604596   \n",
       "16               27           0.663524            0.878538           0.667649   \n",
       "17               23           0.693577            0.910705           0.693577   \n",
       "18               14           0.747201            0.933766           0.736005   \n",
       "19               10           0.773718            0.948244           0.758986   \n",
       "20                6           0.796111            0.957613           0.774308   \n",
       "21               38           0.535062            0.772209           0.525633   \n",
       "22               31           0.635239            0.880372           0.639364   \n",
       "23               24           0.684148            0.923415           0.698291   \n",
       "24               18           0.720094            0.946803           0.724808   \n",
       "25               12           0.758397            0.965671           0.753683   \n",
       "26                8           0.765468            0.974384           0.776665   \n",
       "27                4           0.789629            0.978774           0.787861   \n",
       "28               36           0.549794            0.830123           0.533883   \n",
       "29               28           0.653506            0.917191           0.667060   \n",
       "30               22           0.695345            0.952503           0.723041   \n",
       "31               16           0.728933            0.970846           0.741308   \n",
       "32                9           0.780790            0.980870           0.768415   \n",
       "33                5           0.802004            0.986832           0.786093   \n",
       "34                2           0.806718            0.988994           0.799057   \n",
       "35               37           0.532705            0.861701           0.532115   \n",
       "36               29           0.637006            0.944182           0.653506   \n",
       "37               21           0.692398            0.972681           0.715969   \n",
       "38               15           0.725987            0.983491           0.743076   \n",
       "39                7           0.775486            0.990959           0.772540   \n",
       "40                3           0.789629            0.993449           0.797879   \n",
       "41                1           0.800236            0.994693           0.806718   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0             0.495545           0.384207            0.472353      0.081700   \n",
       "1             0.634762           0.492045            0.618252      0.125276   \n",
       "2             0.705516           0.555097            0.696868      0.084782   \n",
       "3             0.751310           0.607543            0.747248      0.061035   \n",
       "4             0.810273           0.675309            0.811452      0.071779   \n",
       "5             0.839557           0.705952            0.832744      0.294505   \n",
       "6             0.854232           0.730112            0.852660      0.138987   \n",
       "7             0.599515           0.459635            0.600891      0.044893   \n",
       "8             0.749017           0.574543            0.748297      0.065387   \n",
       "9             0.812959           0.644667            0.820558      0.172783   \n",
       "10            0.850956           0.685916            0.856067      0.127854   \n",
       "11            0.892361           0.748379            0.897406      0.282430   \n",
       "12            0.910443           0.760754            0.913653      0.253957   \n",
       "13            0.918436           0.785504            0.925314      0.264881   \n",
       "14            0.687828           0.473777            0.684355      0.795735   \n",
       "15            0.820362           0.601650            0.818920      0.105300   \n",
       "16            0.880831           0.668238            0.877293      0.201872   \n",
       "17            0.912343           0.710666            0.907429      1.048325   \n",
       "18            0.939531           0.746612            0.936648      0.273186   \n",
       "19            0.950668           0.776665            0.949161      0.534332   \n",
       "20            0.957023           0.788450            0.958333      0.170681   \n",
       "21            0.755962           0.491456            0.750852      0.674180   \n",
       "22            0.874410           0.616382            0.877752      0.190985   \n",
       "23            0.923808           0.681791            0.926363      0.033240   \n",
       "24            0.945689           0.733648            0.946541      0.065602   \n",
       "25            0.963968           0.772540            0.964688      0.161436   \n",
       "26            0.972419           0.787861            0.975301      0.077515   \n",
       "27            0.977136           0.800825            0.979036      0.998968   \n",
       "28            0.798546           0.528580            0.803525      0.136971   \n",
       "29            0.904481           0.654685            0.915160      0.041833   \n",
       "30            0.943265           0.698291            0.950341      0.075408   \n",
       "31            0.964819           0.741308            0.965867      0.048445   \n",
       "32            0.977725           0.772540            0.978053      0.156272   \n",
       "33            0.982835           0.789629            0.985194      0.127502   \n",
       "34            0.985718           0.806718            0.987945      1.254174   \n",
       "35            0.841916           0.531526            0.849188      0.172407   \n",
       "36            0.935469           0.647613            0.943527      0.659774   \n",
       "37            0.967309           0.709487            0.970060      0.126266   \n",
       "38            0.979756           0.745433            0.980805      0.792023   \n",
       "39            0.989321           0.787861            0.986701      0.132610   \n",
       "40            0.992859           0.805539            0.991025      0.155441   \n",
       "41            0.993776           0.814968            0.991942      0.082882   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "0         0.006569        0.019469         0.010767  \n",
       "1         0.010700        0.016881         0.006864  \n",
       "2         0.008742        0.014957         0.003534  \n",
       "3         0.012219        0.012185         0.003079  \n",
       "4         0.001413        0.000962         0.006103  \n",
       "5         0.049824        0.004445         0.004202  \n",
       "6         0.003589        0.003204         0.004096  \n",
       "7         0.004418        0.004194         0.004345  \n",
       "8         0.009129        0.001547         0.001285  \n",
       "9         0.002748        0.011710         0.004005  \n",
       "10        0.004686        0.005008         0.002811  \n",
       "11        0.062495        0.012213         0.002887  \n",
       "12        0.063661        0.007562         0.001513  \n",
       "13        0.051579        0.015339         0.002925  \n",
       "14        0.020983        0.014822         0.009602  \n",
       "15        0.002239        0.003675         0.005007  \n",
       "16        0.002367        0.002097         0.001465  \n",
       "17        0.000893        0.008056         0.002043  \n",
       "18        0.000497        0.005145         0.002354  \n",
       "19        0.069268        0.007733         0.000999  \n",
       "20        0.060580        0.009031         0.000536  \n",
       "21        0.010698        0.018734         0.009106  \n",
       "22        0.003909        0.010004         0.002440  \n",
       "23        0.001188        0.007286         0.001307  \n",
       "24        0.004193        0.005618         0.000475  \n",
       "25        0.004547        0.008013         0.000698  \n",
       "26        0.064190        0.009142         0.001202  \n",
       "27        0.047472        0.005740         0.000841  \n",
       "28        0.012647        0.009014         0.013862  \n",
       "29        0.005858        0.006130         0.005575  \n",
       "30        0.002524        0.012420         0.003945  \n",
       "31        0.000636        0.005834         0.002629  \n",
       "32        0.072542        0.005145         0.001412  \n",
       "33        0.000459        0.006821         0.001640  \n",
       "34        0.018957        0.003611         0.001366  \n",
       "35        0.011259        0.000481         0.008171  \n",
       "36        0.020343        0.006827         0.003962  \n",
       "37        0.023520        0.009942         0.002193  \n",
       "38        0.023083        0.008665         0.001573  \n",
       "39        0.001527        0.006638         0.001754  \n",
       "40        0.019988        0.006497         0.001032  \n",
       "41        0.021130        0.006029         0.001144  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Observation for further experiment\n",
    "\n",
    "We notice that in the above experiment the best results are obtained for the highest value of parameters thus suggesting to try even higher values. Using a higher but narrower range this time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=0, test_size=0.1, train_size=None),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=8, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [40, 50, 60, 70, 80, 90], 'max_depth': [60, 70, 80, 90, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid2= {'max_depth': [60, 70, 80,90,100],\n",
    "              'n_estimators': [40, 50, 60,70,80,90]}\n",
    "clf2 = GridSearchCV(RandomForestClassifier(random_state=0 , n_jobs = 8), param_grid2 , cv = cv)\n",
    "%time clf2.fit(vectors_cv, labels_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results2 = pd.DataFrame(clf2.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.025961</td>\n",
       "      <td>0.213422</td>\n",
       "      <td>0.792575</td>\n",
       "      <td>0.984954</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 60}</td>\n",
       "      <td>30</td>\n",
       "      <td>0.802004</td>\n",
       "      <td>0.986832</td>\n",
       "      <td>0.786093</td>\n",
       "      <td>0.982835</td>\n",
       "      <td>0.789629</td>\n",
       "      <td>0.985194</td>\n",
       "      <td>0.599586</td>\n",
       "      <td>0.065213</td>\n",
       "      <td>0.006821</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.211836</td>\n",
       "      <td>0.281760</td>\n",
       "      <td>0.804164</td>\n",
       "      <td>0.987552</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 60}</td>\n",
       "      <td>27</td>\n",
       "      <td>0.806718</td>\n",
       "      <td>0.988994</td>\n",
       "      <td>0.799057</td>\n",
       "      <td>0.985718</td>\n",
       "      <td>0.806718</td>\n",
       "      <td>0.987945</td>\n",
       "      <td>0.268820</td>\n",
       "      <td>0.070085</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.001366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.134366</td>\n",
       "      <td>0.379295</td>\n",
       "      <td>0.812610</td>\n",
       "      <td>0.988753</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>{'n_estimators': 60, 'max_depth': 60}</td>\n",
       "      <td>21</td>\n",
       "      <td>0.814378</td>\n",
       "      <td>0.988994</td>\n",
       "      <td>0.809664</td>\n",
       "      <td>0.987618</td>\n",
       "      <td>0.813789</td>\n",
       "      <td>0.989649</td>\n",
       "      <td>0.494110</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.000846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.383046</td>\n",
       "      <td>0.319809</td>\n",
       "      <td>0.819093</td>\n",
       "      <td>0.989780</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "      <td>{'n_estimators': 70, 'max_depth': 60}</td>\n",
       "      <td>18</td>\n",
       "      <td>0.823217</td>\n",
       "      <td>0.990304</td>\n",
       "      <td>0.817914</td>\n",
       "      <td>0.988863</td>\n",
       "      <td>0.816146</td>\n",
       "      <td>0.990173</td>\n",
       "      <td>0.506953</td>\n",
       "      <td>0.063684</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.452258</td>\n",
       "      <td>0.319094</td>\n",
       "      <td>0.826164</td>\n",
       "      <td>0.990719</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 60}</td>\n",
       "      <td>14</td>\n",
       "      <td>0.827932</td>\n",
       "      <td>0.990632</td>\n",
       "      <td>0.822039</td>\n",
       "      <td>0.990042</td>\n",
       "      <td>0.828521</td>\n",
       "      <td>0.991483</td>\n",
       "      <td>0.430513</td>\n",
       "      <td>0.059957</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.000592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17.555641</td>\n",
       "      <td>0.315719</td>\n",
       "      <td>0.831664</td>\n",
       "      <td>0.991549</td>\n",
       "      <td>60</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 60}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.836181</td>\n",
       "      <td>0.992204</td>\n",
       "      <td>0.824985</td>\n",
       "      <td>0.990501</td>\n",
       "      <td>0.833824</td>\n",
       "      <td>0.991942</td>\n",
       "      <td>0.572023</td>\n",
       "      <td>0.053112</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>0.000749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11.490247</td>\n",
       "      <td>0.200874</td>\n",
       "      <td>0.797682</td>\n",
       "      <td>0.992444</td>\n",
       "      <td>70</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 70}</td>\n",
       "      <td>28</td>\n",
       "      <td>0.789629</td>\n",
       "      <td>0.993449</td>\n",
       "      <td>0.797879</td>\n",
       "      <td>0.992859</td>\n",
       "      <td>0.805539</td>\n",
       "      <td>0.991025</td>\n",
       "      <td>0.057396</td>\n",
       "      <td>0.053809</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>0.001032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.546562</td>\n",
       "      <td>0.249444</td>\n",
       "      <td>0.807307</td>\n",
       "      <td>0.993470</td>\n",
       "      <td>70</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 70}</td>\n",
       "      <td>24</td>\n",
       "      <td>0.800236</td>\n",
       "      <td>0.994693</td>\n",
       "      <td>0.806718</td>\n",
       "      <td>0.993776</td>\n",
       "      <td>0.814968</td>\n",
       "      <td>0.991942</td>\n",
       "      <td>0.243048</td>\n",
       "      <td>0.022790</td>\n",
       "      <td>0.006029</td>\n",
       "      <td>0.001144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17.747160</td>\n",
       "      <td>0.345111</td>\n",
       "      <td>0.818503</td>\n",
       "      <td>0.994082</td>\n",
       "      <td>70</td>\n",
       "      <td>60</td>\n",
       "      <td>{'n_estimators': 60, 'max_depth': 70}</td>\n",
       "      <td>19</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>0.995086</td>\n",
       "      <td>0.818503</td>\n",
       "      <td>0.994169</td>\n",
       "      <td>0.823807</td>\n",
       "      <td>0.992990</td>\n",
       "      <td>0.877132</td>\n",
       "      <td>0.073292</td>\n",
       "      <td>0.004330</td>\n",
       "      <td>0.000858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19.579997</td>\n",
       "      <td>0.281938</td>\n",
       "      <td>0.825378</td>\n",
       "      <td>0.994759</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>{'n_estimators': 70, 'max_depth': 70}</td>\n",
       "      <td>15</td>\n",
       "      <td>0.823217</td>\n",
       "      <td>0.995480</td>\n",
       "      <td>0.825575</td>\n",
       "      <td>0.994824</td>\n",
       "      <td>0.827342</td>\n",
       "      <td>0.993973</td>\n",
       "      <td>1.045243</td>\n",
       "      <td>0.069548</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.374186</td>\n",
       "      <td>0.367963</td>\n",
       "      <td>0.831074</td>\n",
       "      <td>0.995130</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 70}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.827342</td>\n",
       "      <td>0.995742</td>\n",
       "      <td>0.829699</td>\n",
       "      <td>0.994955</td>\n",
       "      <td>0.836181</td>\n",
       "      <td>0.994693</td>\n",
       "      <td>0.856798</td>\n",
       "      <td>0.096996</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.000445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22.895077</td>\n",
       "      <td>0.397684</td>\n",
       "      <td>0.838735</td>\n",
       "      <td>0.995305</td>\n",
       "      <td>70</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 70}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.832646</td>\n",
       "      <td>0.995545</td>\n",
       "      <td>0.837360</td>\n",
       "      <td>0.995349</td>\n",
       "      <td>0.846199</td>\n",
       "      <td>0.995021</td>\n",
       "      <td>0.477978</td>\n",
       "      <td>0.065537</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.347359</td>\n",
       "      <td>0.201149</td>\n",
       "      <td>0.796307</td>\n",
       "      <td>0.995763</td>\n",
       "      <td>80</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 80}</td>\n",
       "      <td>29</td>\n",
       "      <td>0.796700</td>\n",
       "      <td>0.996659</td>\n",
       "      <td>0.791397</td>\n",
       "      <td>0.995676</td>\n",
       "      <td>0.800825</td>\n",
       "      <td>0.994955</td>\n",
       "      <td>0.665664</td>\n",
       "      <td>0.055494</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>0.000698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.852067</td>\n",
       "      <td>0.233511</td>\n",
       "      <td>0.806521</td>\n",
       "      <td>0.996659</td>\n",
       "      <td>80</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 80}</td>\n",
       "      <td>25</td>\n",
       "      <td>0.807896</td>\n",
       "      <td>0.997117</td>\n",
       "      <td>0.800236</td>\n",
       "      <td>0.997183</td>\n",
       "      <td>0.811432</td>\n",
       "      <td>0.995676</td>\n",
       "      <td>0.387311</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.000695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17.686255</td>\n",
       "      <td>0.341814</td>\n",
       "      <td>0.814771</td>\n",
       "      <td>0.997074</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>{'n_estimators': 60, 'max_depth': 80}</td>\n",
       "      <td>20</td>\n",
       "      <td>0.817325</td>\n",
       "      <td>0.997248</td>\n",
       "      <td>0.805539</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>0.821450</td>\n",
       "      <td>0.996593</td>\n",
       "      <td>0.647795</td>\n",
       "      <td>0.075962</td>\n",
       "      <td>0.006742</td>\n",
       "      <td>0.000344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20.224815</td>\n",
       "      <td>0.376749</td>\n",
       "      <td>0.827539</td>\n",
       "      <td>0.997445</td>\n",
       "      <td>80</td>\n",
       "      <td>70</td>\n",
       "      <td>{'n_estimators': 70, 'max_depth': 80}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.829110</td>\n",
       "      <td>0.997379</td>\n",
       "      <td>0.820860</td>\n",
       "      <td>0.997642</td>\n",
       "      <td>0.832646</td>\n",
       "      <td>0.997314</td>\n",
       "      <td>0.438943</td>\n",
       "      <td>0.019710</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22.620149</td>\n",
       "      <td>0.345794</td>\n",
       "      <td>0.833039</td>\n",
       "      <td>0.997576</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 80}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.833235</td>\n",
       "      <td>0.997248</td>\n",
       "      <td>0.826164</td>\n",
       "      <td>0.997707</td>\n",
       "      <td>0.839717</td>\n",
       "      <td>0.997773</td>\n",
       "      <td>0.771631</td>\n",
       "      <td>0.010366</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.000233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25.049169</td>\n",
       "      <td>0.434645</td>\n",
       "      <td>0.835985</td>\n",
       "      <td>0.997860</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 80}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.833824</td>\n",
       "      <td>0.997510</td>\n",
       "      <td>0.832646</td>\n",
       "      <td>0.998035</td>\n",
       "      <td>0.841485</td>\n",
       "      <td>0.998035</td>\n",
       "      <td>1.094069</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.114398</td>\n",
       "      <td>0.177085</td>\n",
       "      <td>0.808289</td>\n",
       "      <td>0.997598</td>\n",
       "      <td>90</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 90}</td>\n",
       "      <td>23</td>\n",
       "      <td>0.804361</td>\n",
       "      <td>0.998362</td>\n",
       "      <td>0.810253</td>\n",
       "      <td>0.997445</td>\n",
       "      <td>0.810253</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.561295</td>\n",
       "      <td>0.073692</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.000572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16.507921</td>\n",
       "      <td>0.263284</td>\n",
       "      <td>0.812021</td>\n",
       "      <td>0.998275</td>\n",
       "      <td>90</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 90}</td>\n",
       "      <td>22</td>\n",
       "      <td>0.819682</td>\n",
       "      <td>0.998493</td>\n",
       "      <td>0.802593</td>\n",
       "      <td>0.998297</td>\n",
       "      <td>0.813789</td>\n",
       "      <td>0.998035</td>\n",
       "      <td>0.522164</td>\n",
       "      <td>0.020623</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.599260</td>\n",
       "      <td>0.272431</td>\n",
       "      <td>0.821842</td>\n",
       "      <td>0.998340</td>\n",
       "      <td>90</td>\n",
       "      <td>60</td>\n",
       "      <td>{'n_estimators': 60, 'max_depth': 90}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.822039</td>\n",
       "      <td>0.998559</td>\n",
       "      <td>0.814968</td>\n",
       "      <td>0.998297</td>\n",
       "      <td>0.828521</td>\n",
       "      <td>0.998166</td>\n",
       "      <td>0.810429</td>\n",
       "      <td>0.050471</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23.123424</td>\n",
       "      <td>0.282924</td>\n",
       "      <td>0.829110</td>\n",
       "      <td>0.998755</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>{'n_estimators': 70, 'max_depth': 90}</td>\n",
       "      <td>12</td>\n",
       "      <td>0.829110</td>\n",
       "      <td>0.998559</td>\n",
       "      <td>0.821450</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>0.836771</td>\n",
       "      <td>0.999017</td>\n",
       "      <td>0.831682</td>\n",
       "      <td>0.066524</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25.080649</td>\n",
       "      <td>0.444063</td>\n",
       "      <td>0.833039</td>\n",
       "      <td>0.998733</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 90}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.833235</td>\n",
       "      <td>0.998559</td>\n",
       "      <td>0.826164</td>\n",
       "      <td>0.998755</td>\n",
       "      <td>0.839717</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>0.404183</td>\n",
       "      <td>0.070223</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>27.615274</td>\n",
       "      <td>0.408536</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.998843</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 90}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.838539</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>0.823807</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>0.840896</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>0.432787</td>\n",
       "      <td>0.121449</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17.132351</td>\n",
       "      <td>0.253354</td>\n",
       "      <td>0.805932</td>\n",
       "      <td>0.998843</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 100}</td>\n",
       "      <td>26</td>\n",
       "      <td>0.806128</td>\n",
       "      <td>0.999083</td>\n",
       "      <td>0.799057</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>0.812610</td>\n",
       "      <td>0.998493</td>\n",
       "      <td>0.469832</td>\n",
       "      <td>0.018623</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18.696790</td>\n",
       "      <td>0.290734</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 100}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.829699</td>\n",
       "      <td>0.999279</td>\n",
       "      <td>0.816735</td>\n",
       "      <td>0.999279</td>\n",
       "      <td>0.826164</td>\n",
       "      <td>0.998821</td>\n",
       "      <td>0.965493</td>\n",
       "      <td>0.061541</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21.304539</td>\n",
       "      <td>0.323326</td>\n",
       "      <td>0.829307</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>{'n_estimators': 60, 'max_depth': 100}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.835003</td>\n",
       "      <td>0.999279</td>\n",
       "      <td>0.819682</td>\n",
       "      <td>0.999148</td>\n",
       "      <td>0.833235</td>\n",
       "      <td>0.999279</td>\n",
       "      <td>0.649121</td>\n",
       "      <td>0.059317</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>24.833774</td>\n",
       "      <td>0.380186</td>\n",
       "      <td>0.832449</td>\n",
       "      <td>0.999258</td>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "      <td>{'n_estimators': 70, 'max_depth': 100}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>0.827932</td>\n",
       "      <td>0.999017</td>\n",
       "      <td>0.835003</td>\n",
       "      <td>0.999410</td>\n",
       "      <td>0.490927</td>\n",
       "      <td>0.014730</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28.503517</td>\n",
       "      <td>0.305826</td>\n",
       "      <td>0.835985</td>\n",
       "      <td>0.999301</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 100}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.840306</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>0.828521</td>\n",
       "      <td>0.999017</td>\n",
       "      <td>0.839128</td>\n",
       "      <td>0.999541</td>\n",
       "      <td>2.251026</td>\n",
       "      <td>0.052562</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.801429</td>\n",
       "      <td>0.401652</td>\n",
       "      <td>0.843449</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 100}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999214</td>\n",
       "      <td>0.838539</td>\n",
       "      <td>0.999214</td>\n",
       "      <td>0.846199</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>1.477846</td>\n",
       "      <td>0.115026</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       10.025961         0.213422         0.792575          0.984954   \n",
       "1       11.211836         0.281760         0.804164          0.987552   \n",
       "2       13.134366         0.379295         0.812610          0.988753   \n",
       "3       15.383046         0.319809         0.819093          0.989780   \n",
       "4       17.452258         0.319094         0.826164          0.990719   \n",
       "5       17.555641         0.315719         0.831664          0.991549   \n",
       "6       11.490247         0.200874         0.797682          0.992444   \n",
       "7       13.546562         0.249444         0.807307          0.993470   \n",
       "8       17.747160         0.345111         0.818503          0.994082   \n",
       "9       19.579997         0.281938         0.825378          0.994759   \n",
       "10      20.374186         0.367963         0.831074          0.995130   \n",
       "11      22.895077         0.397684         0.838735          0.995305   \n",
       "12      13.347359         0.201149         0.796307          0.995763   \n",
       "13      15.852067         0.233511         0.806521          0.996659   \n",
       "14      17.686255         0.341814         0.814771          0.997074   \n",
       "15      20.224815         0.376749         0.827539          0.997445   \n",
       "16      22.620149         0.345794         0.833039          0.997576   \n",
       "17      25.049169         0.434645         0.835985          0.997860   \n",
       "18      14.114398         0.177085         0.808289          0.997598   \n",
       "19      16.507921         0.263284         0.812021          0.998275   \n",
       "20      20.599260         0.272431         0.821842          0.998340   \n",
       "21      23.123424         0.282924         0.829110          0.998755   \n",
       "22      25.080649         0.444063         0.833039          0.998733   \n",
       "23      27.615274         0.408536         0.834414          0.998843   \n",
       "24      17.132351         0.253354         0.805932          0.998843   \n",
       "25      18.696790         0.290734         0.824200          0.999126   \n",
       "26      21.304539         0.323326         0.829307          0.999236   \n",
       "27      24.833774         0.380186         0.832449          0.999258   \n",
       "28      28.503517         0.305826         0.835985          0.999301   \n",
       "29      30.801429         0.401652         0.843449          0.999345   \n",
       "\n",
       "   param_max_depth param_n_estimators                                  params  \\\n",
       "0               60                 40   {'n_estimators': 40, 'max_depth': 60}   \n",
       "1               60                 50   {'n_estimators': 50, 'max_depth': 60}   \n",
       "2               60                 60   {'n_estimators': 60, 'max_depth': 60}   \n",
       "3               60                 70   {'n_estimators': 70, 'max_depth': 60}   \n",
       "4               60                 80   {'n_estimators': 80, 'max_depth': 60}   \n",
       "5               60                 90   {'n_estimators': 90, 'max_depth': 60}   \n",
       "6               70                 40   {'n_estimators': 40, 'max_depth': 70}   \n",
       "7               70                 50   {'n_estimators': 50, 'max_depth': 70}   \n",
       "8               70                 60   {'n_estimators': 60, 'max_depth': 70}   \n",
       "9               70                 70   {'n_estimators': 70, 'max_depth': 70}   \n",
       "10              70                 80   {'n_estimators': 80, 'max_depth': 70}   \n",
       "11              70                 90   {'n_estimators': 90, 'max_depth': 70}   \n",
       "12              80                 40   {'n_estimators': 40, 'max_depth': 80}   \n",
       "13              80                 50   {'n_estimators': 50, 'max_depth': 80}   \n",
       "14              80                 60   {'n_estimators': 60, 'max_depth': 80}   \n",
       "15              80                 70   {'n_estimators': 70, 'max_depth': 80}   \n",
       "16              80                 80   {'n_estimators': 80, 'max_depth': 80}   \n",
       "17              80                 90   {'n_estimators': 90, 'max_depth': 80}   \n",
       "18              90                 40   {'n_estimators': 40, 'max_depth': 90}   \n",
       "19              90                 50   {'n_estimators': 50, 'max_depth': 90}   \n",
       "20              90                 60   {'n_estimators': 60, 'max_depth': 90}   \n",
       "21              90                 70   {'n_estimators': 70, 'max_depth': 90}   \n",
       "22              90                 80   {'n_estimators': 80, 'max_depth': 90}   \n",
       "23              90                 90   {'n_estimators': 90, 'max_depth': 90}   \n",
       "24             100                 40  {'n_estimators': 40, 'max_depth': 100}   \n",
       "25             100                 50  {'n_estimators': 50, 'max_depth': 100}   \n",
       "26             100                 60  {'n_estimators': 60, 'max_depth': 100}   \n",
       "27             100                 70  {'n_estimators': 70, 'max_depth': 100}   \n",
       "28             100                 80  {'n_estimators': 80, 'max_depth': 100}   \n",
       "29             100                 90  {'n_estimators': 90, 'max_depth': 100}   \n",
       "\n",
       "    rank_test_score  split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0                30           0.802004            0.986832           0.786093   \n",
       "1                27           0.806718            0.988994           0.799057   \n",
       "2                21           0.814378            0.988994           0.809664   \n",
       "3                18           0.823217            0.990304           0.817914   \n",
       "4                14           0.827932            0.990632           0.822039   \n",
       "5                 9           0.836181            0.992204           0.824985   \n",
       "6                28           0.789629            0.993449           0.797879   \n",
       "7                24           0.800236            0.994693           0.806718   \n",
       "8                19           0.813200            0.995086           0.818503   \n",
       "9                15           0.823217            0.995480           0.825575   \n",
       "10               10           0.827342            0.995742           0.829699   \n",
       "11                2           0.832646            0.995545           0.837360   \n",
       "12               29           0.796700            0.996659           0.791397   \n",
       "13               25           0.807896            0.997117           0.800236   \n",
       "14               20           0.817325            0.997248           0.805539   \n",
       "15               13           0.829110            0.997379           0.820860   \n",
       "16                6           0.833235            0.997248           0.826164   \n",
       "17                3           0.833824            0.997510           0.832646   \n",
       "18               23           0.804361            0.998362           0.810253   \n",
       "19               22           0.819682            0.998493           0.802593   \n",
       "20               17           0.822039            0.998559           0.814968   \n",
       "21               12           0.829110            0.998559           0.821450   \n",
       "22                6           0.833235            0.998559           0.826164   \n",
       "23                5           0.838539            0.998690           0.823807   \n",
       "24               26           0.806128            0.999083           0.799057   \n",
       "25               16           0.829699            0.999279           0.816735   \n",
       "26               11           0.835003            0.999279           0.819682   \n",
       "27                8           0.834414            0.999345           0.827932   \n",
       "28                3           0.840306            0.999345           0.828521   \n",
       "29                1           0.845610            0.999214           0.838539   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0             0.982835           0.789629            0.985194      0.599586   \n",
       "1             0.985718           0.806718            0.987945      0.268820   \n",
       "2             0.987618           0.813789            0.989649      0.494110   \n",
       "3             0.988863           0.816146            0.990173      0.506953   \n",
       "4             0.990042           0.828521            0.991483      0.430513   \n",
       "5             0.990501           0.833824            0.991942      0.572023   \n",
       "6             0.992859           0.805539            0.991025      0.057396   \n",
       "7             0.993776           0.814968            0.991942      0.243048   \n",
       "8             0.994169           0.823807            0.992990      0.877132   \n",
       "9             0.994824           0.827342            0.993973      1.045243   \n",
       "10            0.994955           0.836181            0.994693      0.856798   \n",
       "11            0.995349           0.846199            0.995021      0.477978   \n",
       "12            0.995676           0.800825            0.994955      0.665664   \n",
       "13            0.997183           0.811432            0.995676      0.387311   \n",
       "14            0.997379           0.821450            0.996593      0.647795   \n",
       "15            0.997642           0.832646            0.997314      0.438943   \n",
       "16            0.997707           0.839717            0.997773      0.771631   \n",
       "17            0.998035           0.841485            0.998035      1.094069   \n",
       "18            0.997445           0.810253            0.996986      0.561295   \n",
       "19            0.998297           0.813789            0.998035      0.522164   \n",
       "20            0.998297           0.828521            0.998166      0.810429   \n",
       "21            0.998690           0.836771            0.999017      0.831682   \n",
       "22            0.998755           0.839717            0.998886      0.404183   \n",
       "23            0.998886           0.840896            0.998952      0.432787   \n",
       "24            0.998952           0.812610            0.998493      0.469832   \n",
       "25            0.999279           0.826164            0.998821      0.965493   \n",
       "26            0.999148           0.833235            0.999279      0.649121   \n",
       "27            0.999017           0.835003            0.999410      0.490927   \n",
       "28            0.999017           0.839128            0.999541      2.251026   \n",
       "29            0.999214           0.846199            0.999607      1.477846   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "0         0.065213        0.006821         0.001640  \n",
       "1         0.070085        0.003611         0.001366  \n",
       "2         0.003539        0.002097         0.000846  \n",
       "3         0.063684        0.003005         0.000651  \n",
       "4         0.059957        0.002927         0.000592  \n",
       "5         0.053112        0.004819         0.000749  \n",
       "6         0.053809        0.006497         0.001032  \n",
       "7         0.022790        0.006029         0.001144  \n",
       "8         0.073292        0.004330         0.000858  \n",
       "9         0.069548        0.001690         0.000617  \n",
       "10        0.096996        0.003737         0.000445  \n",
       "11        0.065537        0.005618         0.000216  \n",
       "12        0.055494        0.003859         0.000698  \n",
       "13        0.002797        0.004673         0.000695  \n",
       "14        0.075962        0.006742         0.000344  \n",
       "15        0.019710        0.004938         0.000142  \n",
       "16        0.010366        0.005535         0.000233  \n",
       "17        0.059524        0.003919         0.000247  \n",
       "18        0.073692        0.002778         0.000572  \n",
       "19        0.020623        0.007088         0.000188  \n",
       "20        0.050471        0.005535         0.000163  \n",
       "21        0.066524        0.006255         0.000193  \n",
       "22        0.070223        0.005535         0.000135  \n",
       "23        0.121449        0.007562         0.000111  \n",
       "24        0.018623        0.005535         0.000253  \n",
       "25        0.061541        0.005472         0.000216  \n",
       "26        0.059317        0.006844         0.000062  \n",
       "27        0.014730        0.003204         0.000172  \n",
       "28        0.052562        0.005300         0.000216  \n",
       "29        0.115026        0.003481         0.000185  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 : Even further experiment\n",
    "\n",
    "The same problem repeats again. Using even higher values but with small range now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=0, test_size=0.1, train_size=None),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=8, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [80, 90, 100, 110], 'max_depth': [90, 100, 110, 120]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid3= {'max_depth': [90,100, 110, 120],\n",
    "              'n_estimators': [80,90, 100 , 110]}\n",
    "clf3 = GridSearchCV(RandomForestClassifier(random_state=0 , n_jobs = 8), param_grid3 , cv = cv)\n",
    "%time clf3.fit(vectors_cv, labels_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.795837</td>\n",
       "      <td>0.411968</td>\n",
       "      <td>0.833039</td>\n",
       "      <td>0.998733</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 90}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.833235</td>\n",
       "      <td>0.998559</td>\n",
       "      <td>0.826164</td>\n",
       "      <td>0.998755</td>\n",
       "      <td>0.839717</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>0.518043</td>\n",
       "      <td>0.049680</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>1.346177e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.082280</td>\n",
       "      <td>0.373256</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.998843</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 90}</td>\n",
       "      <td>14</td>\n",
       "      <td>0.838539</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>0.823807</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>0.840896</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>1.451036</td>\n",
       "      <td>0.141791</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>1.113518e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.292976</td>\n",
       "      <td>0.373348</td>\n",
       "      <td>0.836574</td>\n",
       "      <td>0.998843</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 90}</td>\n",
       "      <td>12</td>\n",
       "      <td>0.836181</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>0.830289</td>\n",
       "      <td>0.998821</td>\n",
       "      <td>0.843253</td>\n",
       "      <td>0.998821</td>\n",
       "      <td>1.037137</td>\n",
       "      <td>0.053993</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.470957</td>\n",
       "      <td>0.402448</td>\n",
       "      <td>0.839128</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>90</td>\n",
       "      <td>110</td>\n",
       "      <td>{'n_estimators': 110, 'max_depth': 90}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.842074</td>\n",
       "      <td>0.999017</td>\n",
       "      <td>0.830289</td>\n",
       "      <td>0.998886</td>\n",
       "      <td>0.845021</td>\n",
       "      <td>0.998952</td>\n",
       "      <td>0.270371</td>\n",
       "      <td>0.101395</td>\n",
       "      <td>0.006365</td>\n",
       "      <td>5.349165e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.376795</td>\n",
       "      <td>0.362958</td>\n",
       "      <td>0.835985</td>\n",
       "      <td>0.999301</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 100}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.840306</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>0.828521</td>\n",
       "      <td>0.999017</td>\n",
       "      <td>0.839128</td>\n",
       "      <td>0.999541</td>\n",
       "      <td>1.018903</td>\n",
       "      <td>0.107333</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>2.161839e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28.527824</td>\n",
       "      <td>0.297910</td>\n",
       "      <td>0.843449</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 100}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999214</td>\n",
       "      <td>0.838539</td>\n",
       "      <td>0.999214</td>\n",
       "      <td>0.846199</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>0.643824</td>\n",
       "      <td>0.050777</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>1.853005e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31.149265</td>\n",
       "      <td>0.459654</td>\n",
       "      <td>0.846003</td>\n",
       "      <td>0.999323</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 100}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.849735</td>\n",
       "      <td>0.999214</td>\n",
       "      <td>0.839717</td>\n",
       "      <td>0.999148</td>\n",
       "      <td>0.848556</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>0.670212</td>\n",
       "      <td>0.113889</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>2.025161e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>38.339701</td>\n",
       "      <td>0.515438</td>\n",
       "      <td>0.848753</td>\n",
       "      <td>0.999476</td>\n",
       "      <td>100</td>\n",
       "      <td>110</td>\n",
       "      <td>{'n_estimators': 110, 'max_depth': 100}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.849735</td>\n",
       "      <td>0.999410</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999279</td>\n",
       "      <td>0.850913</td>\n",
       "      <td>0.999738</td>\n",
       "      <td>1.447922</td>\n",
       "      <td>0.129968</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>1.928669e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30.370284</td>\n",
       "      <td>0.348293</td>\n",
       "      <td>0.834021</td>\n",
       "      <td>0.999716</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 110}</td>\n",
       "      <td>15</td>\n",
       "      <td>0.828521</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.839128</td>\n",
       "      <td>0.999803</td>\n",
       "      <td>0.400752</td>\n",
       "      <td>0.106480</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>6.176684e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32.558777</td>\n",
       "      <td>0.388287</td>\n",
       "      <td>0.838342</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>110</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 110}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.837949</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.842664</td>\n",
       "      <td>0.999738</td>\n",
       "      <td>1.728720</td>\n",
       "      <td>0.071644</td>\n",
       "      <td>0.003379</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>36.614595</td>\n",
       "      <td>0.398460</td>\n",
       "      <td>0.842664</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>110</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 110}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.840896</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.839128</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>0.847967</td>\n",
       "      <td>0.999803</td>\n",
       "      <td>0.765892</td>\n",
       "      <td>0.091244</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>8.170985e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>42.639916</td>\n",
       "      <td>0.448275</td>\n",
       "      <td>0.844038</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>{'n_estimators': 110, 'max_depth': 110}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.839128</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>0.847378</td>\n",
       "      <td>0.999803</td>\n",
       "      <td>1.281271</td>\n",
       "      <td>0.089096</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>8.170985e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>30.400612</td>\n",
       "      <td>0.479428</td>\n",
       "      <td>0.841092</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>{'n_estimators': 80, 'max_depth': 120}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.844431</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.835003</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.843842</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.876327</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.004312</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33.295857</td>\n",
       "      <td>0.396021</td>\n",
       "      <td>0.841485</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>{'n_estimators': 90, 'max_depth': 120}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.844431</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.622665</td>\n",
       "      <td>0.119412</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>35.765307</td>\n",
       "      <td>0.462118</td>\n",
       "      <td>0.844824</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 120}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.836771</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.852092</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.660004</td>\n",
       "      <td>0.104951</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>38.507540</td>\n",
       "      <td>0.539555</td>\n",
       "      <td>0.849146</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>120</td>\n",
       "      <td>110</td>\n",
       "      <td>{'n_estimators': 110, 'max_depth': 120}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.853270</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.841485</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>1.202296</td>\n",
       "      <td>0.071258</td>\n",
       "      <td>0.005422</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       24.795837         0.411968         0.833039          0.998733   \n",
       "1       28.082280         0.373256         0.834414          0.998843   \n",
       "2       30.292976         0.373348         0.836574          0.998843   \n",
       "3       30.470957         0.402448         0.839128          0.998952   \n",
       "4       25.376795         0.362958         0.835985          0.999301   \n",
       "5       28.527824         0.297910         0.843449          0.999345   \n",
       "6       31.149265         0.459654         0.846003          0.999323   \n",
       "7       38.339701         0.515438         0.848753          0.999476   \n",
       "8       30.370284         0.348293         0.834021          0.999716   \n",
       "9       32.558777         0.388287         0.838342          0.999694   \n",
       "10      36.614595         0.398460         0.842664          0.999694   \n",
       "11      42.639916         0.448275         0.844038          0.999694   \n",
       "12      30.400612         0.479428         0.841092          0.999891   \n",
       "13      33.295857         0.396021         0.841485          0.999913   \n",
       "14      35.765307         0.462118         0.844824          0.999913   \n",
       "15      38.507540         0.539555         0.849146          0.999934   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "0               90                 80   \n",
       "1               90                 90   \n",
       "2               90                100   \n",
       "3               90                110   \n",
       "4              100                 80   \n",
       "5              100                 90   \n",
       "6              100                100   \n",
       "7              100                110   \n",
       "8              110                 80   \n",
       "9              110                 90   \n",
       "10             110                100   \n",
       "11             110                110   \n",
       "12             120                 80   \n",
       "13             120                 90   \n",
       "14             120                100   \n",
       "15             120                110   \n",
       "\n",
       "                                     params  rank_test_score  \\\n",
       "0     {'n_estimators': 80, 'max_depth': 90}               16   \n",
       "1     {'n_estimators': 90, 'max_depth': 90}               14   \n",
       "2    {'n_estimators': 100, 'max_depth': 90}               12   \n",
       "3    {'n_estimators': 110, 'max_depth': 90}               10   \n",
       "4    {'n_estimators': 80, 'max_depth': 100}               13   \n",
       "5    {'n_estimators': 90, 'max_depth': 100}                6   \n",
       "6   {'n_estimators': 100, 'max_depth': 100}                3   \n",
       "7   {'n_estimators': 110, 'max_depth': 100}                2   \n",
       "8    {'n_estimators': 80, 'max_depth': 110}               15   \n",
       "9    {'n_estimators': 90, 'max_depth': 110}               11   \n",
       "10  {'n_estimators': 100, 'max_depth': 110}                7   \n",
       "11  {'n_estimators': 110, 'max_depth': 110}                5   \n",
       "12   {'n_estimators': 80, 'max_depth': 120}                9   \n",
       "13   {'n_estimators': 90, 'max_depth': 120}                8   \n",
       "14  {'n_estimators': 100, 'max_depth': 120}                4   \n",
       "15  {'n_estimators': 110, 'max_depth': 120}                1   \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0            0.833235            0.998559           0.826164   \n",
       "1            0.838539            0.998690           0.823807   \n",
       "2            0.836181            0.998886           0.830289   \n",
       "3            0.842074            0.999017           0.830289   \n",
       "4            0.840306            0.999345           0.828521   \n",
       "5            0.845610            0.999214           0.838539   \n",
       "6            0.849735            0.999214           0.839717   \n",
       "7            0.849735            0.999410           0.845610   \n",
       "8            0.828521            0.999672           0.834414   \n",
       "9            0.834414            0.999672           0.837949   \n",
       "10           0.840896            0.999672           0.839128   \n",
       "11           0.845610            0.999672           0.839128   \n",
       "12           0.844431            0.999934           0.835003   \n",
       "13           0.845610            0.999934           0.834414   \n",
       "14           0.845610            0.999934           0.836771   \n",
       "15           0.853270            0.999934           0.841485   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0             0.998755           0.839717            0.998886      0.518043   \n",
       "1             0.998886           0.840896            0.998952      1.451036   \n",
       "2             0.998821           0.843253            0.998821      1.037137   \n",
       "3             0.998886           0.845021            0.998952      0.270371   \n",
       "4             0.999017           0.839128            0.999541      1.018903   \n",
       "5             0.999214           0.846199            0.999607      0.643824   \n",
       "6             0.999148           0.848556            0.999607      0.670212   \n",
       "7             0.999279           0.850913            0.999738      1.447922   \n",
       "8             0.999672           0.839128            0.999803      0.400752   \n",
       "9             0.999672           0.842664            0.999738      1.728720   \n",
       "10            0.999607           0.847967            0.999803      0.765892   \n",
       "11            0.999607           0.847378            0.999803      1.281271   \n",
       "12            0.999869           0.843842            0.999869      0.876327   \n",
       "13            0.999934           0.844431            0.999869      0.622665   \n",
       "14            0.999934           0.852092            0.999869      0.660004   \n",
       "15            0.999934           0.852681            0.999934      1.202296   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "0         0.049680        0.005535     1.346177e-04  \n",
       "1         0.141791        0.007562     1.113518e-04  \n",
       "2         0.053993        0.005300     3.088342e-05  \n",
       "3         0.101395        0.006365     5.349165e-05  \n",
       "4         0.107333        0.005300     2.161839e-04  \n",
       "5         0.050777        0.003481     1.853005e-04  \n",
       "6         0.113889        0.004471     2.025161e-04  \n",
       "7         0.129968        0.002274     1.928669e-04  \n",
       "8         0.106480        0.004339     6.176684e-05  \n",
       "9         0.071644        0.003379     3.088342e-05  \n",
       "10        0.091244        0.003819     8.170985e-05  \n",
       "11        0.089096        0.003547     8.170985e-05  \n",
       "12        0.002250        0.004312     3.088342e-05  \n",
       "13        0.119412        0.005023     3.088342e-05  \n",
       "14        0.104951        0.006279     3.088342e-05  \n",
       "15        0.071258        0.005422     1.110223e-16  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results3 = pd.DataFrame(clf3.cv_results_)\n",
    "results3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 : The final step\n",
    "\n",
    "the same problem still repeats but since the gap in best estimators is much smaller now, this means this step should \n",
    "find the optimal parameters now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=0, test_size=0.1, train_size=None),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=8, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [100, 120, 140], 'max_depth': [120, 140, 160]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid4= {'max_depth': [120,  140, 160],\n",
    "              'n_estimators': [100 ,120, 140]}\n",
    "clf4 = GridSearchCV(RandomForestClassifier(random_state=0 , n_jobs = 8), param_grid4 , cv = cv)\n",
    "%time clf4.fit(vectors_cv, labels_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.849772</td>\n",
       "      <td>0.458792</td>\n",
       "      <td>0.844824</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 120}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.836771</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.852092</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.495320</td>\n",
       "      <td>0.178278</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43.615133</td>\n",
       "      <td>0.452934</td>\n",
       "      <td>0.849735</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>{'n_estimators': 120, 'max_depth': 120}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.855038</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.839717</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.854449</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.231218</td>\n",
       "      <td>0.106040</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.590655</td>\n",
       "      <td>0.535677</td>\n",
       "      <td>0.851306</td>\n",
       "      <td>0.999847</td>\n",
       "      <td>120</td>\n",
       "      <td>140</td>\n",
       "      <td>{'n_estimators': 140, 'max_depth': 120}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.856806</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.843253</td>\n",
       "      <td>0.999803</td>\n",
       "      <td>0.853860</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.553527</td>\n",
       "      <td>0.153332</td>\n",
       "      <td>0.005820</td>\n",
       "      <td>3.088342e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.536621</td>\n",
       "      <td>0.467547</td>\n",
       "      <td>0.848949</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>140</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 140}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.852092</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.846788</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.847967</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.390505</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.254158</td>\n",
       "      <td>0.654150</td>\n",
       "      <td>0.851699</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>140</td>\n",
       "      <td>120</td>\n",
       "      <td>{'n_estimators': 120, 'max_depth': 140}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857395</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.848556</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.849146</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>1.298463</td>\n",
       "      <td>0.244252</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57.783405</td>\n",
       "      <td>0.574911</td>\n",
       "      <td>0.855431</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>{'n_estimators': 140, 'max_depth': 140}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.855628</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.853270</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.857395</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>1.499260</td>\n",
       "      <td>0.167357</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41.718227</td>\n",
       "      <td>0.460633</td>\n",
       "      <td>0.846003</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>160</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 160}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.848556</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.843253</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.846199</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.664722</td>\n",
       "      <td>0.061454</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56.259005</td>\n",
       "      <td>0.750183</td>\n",
       "      <td>0.850913</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>160</td>\n",
       "      <td>120</td>\n",
       "      <td>{'n_estimators': 120, 'max_depth': 160}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.846788</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.853270</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>2.127975</td>\n",
       "      <td>0.042717</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62.502033</td>\n",
       "      <td>0.593865</td>\n",
       "      <td>0.851896</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>160</td>\n",
       "      <td>140</td>\n",
       "      <td>{'n_estimators': 140, 'max_depth': 160}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.857395</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>4.680417</td>\n",
       "      <td>0.111873</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>1.110223e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0      36.849772         0.458792         0.844824          0.999913   \n",
       "1      43.615133         0.452934         0.849735          0.999869   \n",
       "2      48.590655         0.535677         0.851306          0.999847   \n",
       "3      39.536621         0.467547         0.848949          0.999934   \n",
       "4      51.254158         0.654150         0.851699          0.999934   \n",
       "5      57.783405         0.574911         0.855431          0.999934   \n",
       "6      41.718227         0.460633         0.846003          0.999934   \n",
       "7      56.259005         0.750183         0.850913          0.999934   \n",
       "8      62.502033         0.593865         0.851896          0.999934   \n",
       "\n",
       "  param_max_depth param_n_estimators                                   params  \\\n",
       "0             120                100  {'n_estimators': 100, 'max_depth': 120}   \n",
       "1             120                120  {'n_estimators': 120, 'max_depth': 120}   \n",
       "2             120                140  {'n_estimators': 140, 'max_depth': 120}   \n",
       "3             140                100  {'n_estimators': 100, 'max_depth': 140}   \n",
       "4             140                120  {'n_estimators': 120, 'max_depth': 140}   \n",
       "5             140                140  {'n_estimators': 140, 'max_depth': 140}   \n",
       "6             160                100  {'n_estimators': 100, 'max_depth': 160}   \n",
       "7             160                120  {'n_estimators': 120, 'max_depth': 160}   \n",
       "8             160                140  {'n_estimators': 140, 'max_depth': 160}   \n",
       "\n",
       "   rank_test_score  split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0                9           0.845610            0.999934           0.836771   \n",
       "1                6           0.855038            0.999869           0.839717   \n",
       "2                4           0.856806            0.999869           0.843253   \n",
       "3                7           0.852092            0.999934           0.846788   \n",
       "4                3           0.857395            0.999934           0.848556   \n",
       "5                1           0.855628            0.999934           0.853270   \n",
       "6                8           0.848556            0.999934           0.843253   \n",
       "7                5           0.852681            0.999934           0.846788   \n",
       "8                2           0.852681            0.999934           0.845610   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.999934           0.852092            0.999869      0.495320   \n",
       "1            0.999869           0.854449            0.999869      0.231218   \n",
       "2            0.999803           0.853860            0.999869      0.553527   \n",
       "3            0.999934           0.847967            0.999934      0.390505   \n",
       "4            0.999934           0.849146            0.999934      1.298463   \n",
       "5            0.999934           0.857395            0.999934      1.499260   \n",
       "6            0.999934           0.846199            0.999934      0.664722   \n",
       "7            0.999934           0.853270            0.999934      2.127975   \n",
       "8            0.999934           0.857395            0.999934      4.680417   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.178278        0.006279     3.088342e-05  \n",
       "1        0.106040        0.007088     0.000000e+00  \n",
       "2        0.153332        0.005820     3.088342e-05  \n",
       "3        0.025862        0.002274     1.110223e-16  \n",
       "4        0.244252        0.004035     1.110223e-16  \n",
       "5        0.167357        0.001690     1.110223e-16  \n",
       "6        0.061454        0.002170     1.110223e-16  \n",
       "7        0.042717        0.002927     1.110223e-16  \n",
       "8        0.111873        0.004843     1.110223e-16  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results4 = pd.DataFrame(clf4.cv_results_)\n",
    "results4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that now the best performance is found at intermediate values suggesting good paramaters to be found.\n",
    "We can still optimize the values in smaller range but this is not worth the effort since the change from the peak performance\n",
    "around the optimal paramater is too slow that suggests that any such optimization is not fruitful for the effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 : The Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the cross validation is perfomed we can load the model directly with `model = clf4.best_estimator_` \n",
    "Otherwise, we can train the model using the best parameters in the 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=140, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=140, n_jobs=8, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Noting the best parameter setting to be n_estimators = 140 and max_depth = 140. We can directly train \n",
    "# the best model for next time.\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=8, random_state=0 , n_estimators = 140 , max_depth = 140)\n",
    "\n",
    "vectors_train, vector_val,labels_train, labels_val = train_test_split(vectors_cv, labels_cv, test_size=0.11, random_state=42)\n",
    "model.fit(vectors_train,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 : Evaluation Criterion : Accuracy and Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 : Different Accuracies:\n",
    "\n",
    "It is interesting to note that even though the validation dataset is not use for training, the accuracy on val is certainly \n",
    "consistently a bit higher than the testing dataset which is an artefact of the fact that we are using it to indirectly train by \n",
    "choosing the best parameters through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 99.993\n",
      "Validation set Accuracy 85.423\n",
      "Test Accuracy 84.297\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy', np.round(model.score(vectors_train, labels_train)*100,3))\n",
    "print('Validation set Accuracy', np.round(model.score(vector_val,labels_val)*100,3))\n",
    "print('Test Accuracy', np.round(model.score(vector_test,labels_test)*100,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 : Confusion Matrix :\n",
    "\n",
    "We are using the library `pandas-ml` for firectly plotting the confusion matrix. Same can also be achieved in scikit-learn although wit much more lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x568190ef98>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAHZCAYAAAD5SiP6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5682e832b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_pred =model.predict(vector_test)\n",
    "cm = ConfusionMatrix(labels_test,labels_pred)\n",
    "cm.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.5 : Inspecting feature importances : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173762\n"
     ]
    }
   ],
   "source": [
    "print(len(model.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = list(data_all.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.00040000000000000002, 'alt.atheism'), (0.00020000000000000001, 'comp.graphics'), (0.0, 'talk.religion.misc'), (0.0, 'talk.politics.misc'), (0.0, 'talk.politics.mideast'), (0.0, 'talk.politics.guns'), (0.0, 'soc.religion.christian'), (0.0, 'sci.space'), (0.0, 'sci.med'), (0.0, 'sci.electronics'), (0.0, 'sci.crypt'), (0.0, 'rec.sport.hockey'), (0.0, 'rec.sport.baseball'), (0.0, 'rec.motorcycles'), (0.0, 'rec.autos'), (0.0, 'misc.forsale'), (0.0, 'comp.windows.x'), (0.0, 'comp.sys.mac.hardware'), (0.0, 'comp.sys.ibm.pc.hardware'), (0.0, 'comp.os.ms-windows.misc')]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(zip(map(lambda x: round(x, 4), model.feature_importances_), names), \n",
    "             reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
